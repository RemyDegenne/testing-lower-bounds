\section{f-divergences}

\subsection{Definition and basic properties}

Everywhere in this document, the functions used in divergences are assumed measurable, continuous and convex on $[0, +\infty)$.
For such a function $f : \mathbb{R} \to \mathbb{R}$, we write $f'(\infty) = \lim_{x \to +\infty} f(x)/x$ (which can be infinite).

For $\mu, \nu$ two measures, we denote by $\mu_{\perp \nu}$ the singular part of $\mu$ with respect to $\nu$.

\begin{definition}[f-divergence]
  \label{def:fDiv}
  \lean{ProbabilityTheory.fDiv}
  \leanok
  \uses{def:derivAtTop}
  Let $f : \mathbb{R} \to \mathbb{R}$ and let $\mu, \nu$ be two measures on a measurable space $\mathcal X$. The f-divergence between $\mu$ and $\nu$ is
  \begin{align*}
  D_f(\mu, \nu) = \nu\left[x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)\right] + f'(\infty) \mu_{\perp \nu}(\mathcal X)
  \end{align*}
  if $x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)$ is $\nu$-integrable and $+\infty$ otherwise.
\end{definition}


\begin{lemma}
  \label{lem:fDiv_ne_top_iff}
  \lean{ProbabilityTheory.fDiv_ne_top_iff}
  \leanok
  \uses{def:fDiv}
  For $\mu$ and $\nu$ two finite measures, $D_f(\mu, \nu)$ is finite if and only if $x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)$ is $\nu$-integrable and either $f'(\infty) < \infty$ or $\mu \ll \nu$.
\end{lemma}

\begin{proof}\leanok
\end{proof}



\paragraph{Divergences associated with basic (transformations of) functions}

\begin{lemma}
  \label{lem:fDiv_const}
  \lean{ProbabilityTheory.fDiv_const}
  \leanok
  \uses{def:fDiv}
  For $\nu$ a finite measure, for all $a \in \mathbb{R}$, $D_{x \mapsto a}(\mu, \nu) = a \nu(\mathcal X)$.
\end{lemma}

\begin{proof} \leanok
Compute the integral.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_self}
  \lean{ProbabilityTheory.fDiv_self}
  \leanok
  \uses{def:fDiv}
  If $f(1) = 0$ then $D_{f}(\mu, \mu) = 0$.
\end{lemma}

\begin{proof} \leanok
$\frac{d \mu}{d \mu}(x) = 1$ almost everywhere and $f(1) = 0$.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_eq_zero_iff}
  \lean{ProbabilityTheory.fDiv_eq_zero_iff'}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two probability measures. Assume $f'(\infty) = + \infty$ and $f(1) = 0$. Then $D_f(\mu, \nu) = 0$ if and only if $\mu = \nu$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_self}
If $\mu = \nu$ then $D_f(\mu, \nu) = 0$ by Lemma~\ref{lem:fDiv_self}.
For the other direction use the strict version of Jensen's inequality.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_id}
  \lean{ProbabilityTheory.fDiv_id}
  \leanok
  \uses{def:fDiv}
  $D_{x \mapsto x}(\mu, \nu) = \mu(\mathcal X)$.
\end{lemma}

\begin{proof} \leanok
Compute the integral: its value is $(\frac{d\mu}{d\nu}\cdot \nu)(\mathcal X)$. Then
$D_{x\mapsto x}(\mu, \nu) = (\frac{d\mu}{d\nu}\cdot \nu)(\mathcal X) + \mu_{\perp \nu}(\mathcal X) = \mu (\mathcal X)$.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_mul}
  \lean{ProbabilityTheory.fDiv_mul}
  \leanok
  \uses{def:fDiv}
  For all $a \ge 0$, $D_{a f}(\mu, \nu) = a D_{f}(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
Linearity of the integral.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_add}
  \lean{ProbabilityTheory.fDiv_add}
  \leanok
  \uses{def:fDiv}
  $D_{f + g}(\mu, \nu) = D_f(\mu, \nu) + D_g(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
Linearity of the integral.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_add_linear}
  \lean{ProbabilityTheory.fDiv_add_linear}
  \leanok
  \uses{def:fDiv}
  For finite measures $\mu$ and $\nu$ with $\mu(\mathcal X) = \nu(\mathcal X)$, for all $a \in \mathbb{R}$, $D_{f + a(x - 1)}(\mu, \nu) = D_{f}(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_add, lem:fDiv_mul, lem:fDiv_const, lem:fDiv_id}
Linearity (Lemmas~\ref{lem:fDiv_add} and~\ref{lem:fDiv_mul}), then Lemma~\ref{lem:fDiv_const} and~\ref{lem:fDiv_id}.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_map_measurableEmbedding}
  \lean{ProbabilityTheory.fDiv_map_measurableEmbedding}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two measures on $\mathcal X$ and let $g : \mathcal X \to \mathcal Y$ be a measurable embedding. Then $D_f(g_* \mu, g_* \nu) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\end{proof}


\begin{lemma}
  \label{lem:fDiv_symm}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  $D_f(\mu, \nu) = D_{x \mapsto xf(1/x)}(\nu, \mu)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}


\begin{lemma}
  \label{lem:fDiv_add_smul_left}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  $(\mu, \nu) \mapsto D_f(a \mu + b \nu, \nu)$ is an $f$-divergence for the function $x \mapsto f(ax + b)$
\end{lemma}

\begin{proof}%\leanok
\uses{}
\end{proof}


\begin{lemma}
  \label{lem:fDiv_add_smul_right}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  $(\mu, \nu) \mapsto D_f(\mu, a \mu + b \nu)$ is an $f$-divergence for the function $x \mapsto (ax+b)f\left(\frac{x}{ax+b}\right)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:fDiv_symm, lem:fDiv_add_smul_left}
We use Lemma~\ref{lem:fDiv_symm}, then Lemma~\ref{lem:fDiv_add_smul_left} and finally Lemma~\ref{lem:fDiv_symm} again:
\begin{align*}
D_f(\mu, a \mu + b \nu)
&= D_{xf(1/x)}(a \mu + b \nu, \mu)
\\
&= D_{(bx+a)f(1/(bx+a))}(\nu, \mu)
\\
&= D_{(ax+b)f(x/(ax+b))}(\mu, \nu)
\: .
\end{align*}

\end{proof}


\begin{lemma}
  \label{lem:fDiv_add_smul_right'}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  $(\mu, \nu) \mapsto D_f(\nu, a \mu + b \nu)$ is an $f$-divergence for the function $x \mapsto (ax+b)f\left(\frac{1}{ax+b}\right)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:fDiv_add_smul_right, lem:fDiv_symm}
We use Lemma~\ref{lem:fDiv_add_smul_right}, then Lemma~\ref{lem:fDiv_symm}.
\begin{align*}
D_f(\nu, a \mu + b \nu)
&= D_{(bx+a)f\left(\frac{x}{bx+a}\right)}(\nu, \mu)
= D_{(ax+b)f\left(\frac{1}{ax+b}\right)}(\mu, \nu)
\: .
\end{align*}
\end{proof}



\paragraph{Upper and lower bounds on $D_f$}

\begin{lemma}
  \label{lem:fDiv_absolutelyContinuous_add_mutuallySingular}
  \lean{ProbabilityTheory.fDiv_absolutelyContinuous_add_mutuallySingular}
  \leanok
  \uses{def:fDiv}
  Let $\mu_1, \mu_2$ and $\nu$ be finite measures on $\mathcal X$, with $\mu_1 \ll \nu$ and $\mu_2 \perp \nu$.
  Then $D_f(\mu_1 + \mu_2, \nu) = D_f(\mu_1, \nu) + \mu_2(\mathcal X) f'(\infty)$.
\end{lemma}

\begin{proof}\leanok
$\frac{d(\mu_1 + \mu_2)}{d \nu} = \frac{d \mu_1}{d \nu}$ a.e. and $(\mu_1 + \mu_2)_{\perp \nu} = \mu_2$.
\end{proof}


\begin{lemma}[Superseded by Lemma~\ref{lem:fDiv_add_measure_le}]
  \label{lem:fDiv_add_measure_le_of_ac}
  \lean{ProbabilityTheory.fDiv_add_measure_le_of_ac}
  \leanok
  \uses{def:fDiv}
  Let $\mu_1, \mu_2, \nu$ be three finite measures on $\mathcal X$ with $\mu_1 \ll \nu$ and $\mu_2 \ll \nu$. Then
  $D_f(\mu_1 + \mu_2, \nu) \le D_f(\mu_1, \nu) + \mu_2(\mathcal X) f'(\infty)$.
\end{lemma}

\begin{proof}\leanok
\begin{align*}
D_f(\mu_1 + \mu_2, \nu)
&= \int_x f \left( \frac{d \mu_1}{d\nu}(x) + \frac{d\mu_2}{d\nu}(x) \right) \partial \nu
\\
&\le \int_x f \left( \frac{d \mu_1}{d\nu}(x) \right) + \frac{d\mu_2}{d\nu}(x) f'(\infty) \partial \nu
\\
&= D_f(\mu_1, \nu) + \mu_2(\mathcal X) f'(\infty)
\: .
\end{align*}
\end{proof}


\begin{lemma}
  \label{lem:fDiv_add_measure_le}
  \lean{ProbabilityTheory.fDiv_add_measure_le}
  \leanok
  \uses{def:fDiv}
  Let $\mu_1, \mu_2, \nu$ be three finite measures on $\mathcal X$. Then
  $D_f(\mu_1 + \mu_2, \nu) \le D_f(\mu_1, \nu) + \mu_2(\mathcal X) f'(\infty)$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_absolutelyContinuous_add_mutuallySingular, lem:fDiv_add_measure_le_of_ac}
From Lemma~\ref{lem:fDiv_absolutelyContinuous_add_mutuallySingular}, then Lemma~\ref{lem:fDiv_add_measure_le_of_ac},
\begin{align*}
D_f(\mu_1 + \mu_2, \nu)
&= D_f(\frac{d\mu_1}{d \nu}\cdot \nu + \frac{d\mu_2}{d \nu}\cdot \nu, \nu) + (\mu_1)_{\perp\nu}(\mathcal X) f'(\infty) + (\mu_2)_{\perp\nu}(\mathcal X) f'(\infty)
\\
&\le D_f(\frac{d\mu_1}{d \nu}\cdot \nu, \nu) + (\frac{d\mu_2}{d \nu}\cdot \nu)(\mathcal X) f'(\infty) + (\mu_1)_{\perp\nu}(\mathcal X) f'(\infty) + (\mu_2)_{\perp\nu}(\mathcal X) f'(\infty)
\\
&= D_f(\mu_1, \nu) + \mu_2(\mathcal X) f'(\infty)
\: .
\end{align*}
\end{proof}


\begin{lemma}
  \label{lem:fDiv_eq_add_withDensity_derivAtTop}
  \lean{ProbabilityTheory.fDiv_eq_add_withDensity_derivAtTop}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two finite measures on $\mathcal X$.
  Then $D_f(\mu, \nu) = D_f(\frac{d\mu}{d\nu}\cdot \nu, \nu) + f'(\infty) \mu_{\perp \nu}(\mathcal X)$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_absolutelyContinuous_add_mutuallySingular}
Apply Lemma~\ref{lem:fDiv_absolutelyContinuous_add_mutuallySingular} to $\frac{d\mu}{d\nu}\cdot \nu$ and $\mu_{\perp \nu}$.
\end{proof}


\begin{lemma}[Superseded by Lemma~\ref{lem:le_fDiv}]
  \label{lem:le_fDiv_of_ac}
  \lean{ProbabilityTheory.le_fDiv_of_ac}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ be a finite measure and $\nu$ be a probability measure on the same space $\mathcal X$, such that $\mu \ll \nu$. Then $f(\mu(\mathcal X)) \le D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
Since $\mu \ll \nu$ the $f$-divergence is only the integral part. Then by Jensen's inequality,
\begin{align*}
D_f(\mu, \nu)
&= \nu\left[ f\left( \frac{d\mu}{d\nu} \right) \right]
\ge f\left( \nu\left[\frac{d\mu}{d\nu} \right] \right)
= f(\mu(\mathcal X))
\: .
\end{align*}

\end{proof}


\begin{lemma}
  \label{lem:le_fDiv}
  \lean{ProbabilityTheory.le_fDiv}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ be a finite measure and $\nu$ be a probability measure on the same space $\mathcal X$. Then $f(\mu(\mathcal X)) \le D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_eq_add_withDensity_derivAtTop, lem:le_fDiv_of_ac}
By convexity, then Lemma~\ref{lem:le_fDiv_of_ac} and finally Lemma~\ref{lem:fDiv_eq_add_withDensity_derivAtTop},
\begin{align*}
f(\mu(\mathcal X))
&\le f(\frac{d\mu}{d\nu}\cdot \nu (\mathcal X)) + f'(\infty)\mu_{\perp}(\mathcal X)
\\
&\le D_f(\frac{d\mu}{d\nu}\cdot \nu , \nu) + f'(\infty)\mu_{\perp}(\mathcal X)
\\
&= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}


\begin{lemma}
  \label{lem:fDiv_nonneg}
  \lean{ProbabilityTheory.fDiv_nonneg}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two probability measures. If $f(1) = 0$ then $D_f(\mu, \nu) \ge 0$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:le_fDiv}
Apply Lemma~\ref{lem:le_fDiv} and use $f(\mu(\mathcal X)) = f(1) = 0$.
\end{proof}



\subsection{Conditional f-divergence}

TODO: replace the definition by $D(\mu \otimes \kappa, \mu \otimes \eta)$, which allows for a more general proof of the ``conditioning increases divergence'' inequality.

\begin{definition}[Conditional f-divergence]
  \label{def:condFDiv}
  \lean{ProbabilityTheory.condFDiv}
  \leanok
  \uses{def:fDiv}
  Let $f : \mathbb{R} \to \mathbb{R}$, $\mu$ a measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ two Markov kernels from $\mathcal X$ to $\mathcal Y$. The conditional f-divergence between $\kappa$ and $\eta$ with respect to $\mu$ is
  \begin{align*}
  D_f(\kappa, \eta \mid \mu) = \mu\left[x \mapsto D_f(\kappa(x), \eta(x))\right]
  \end{align*}
  if $x \mapsto D_f(\kappa(x), \eta(x))$ is $\mu$-integrable and $+\infty$ otherwise.
\end{definition}


\begin{lemma}
  \label{lem:condFDiv_nonneg}
  \lean{ProbabilityTheory.condFDiv_nonneg}
  \leanok
  \uses{def:condFDiv}
  Let $\mu$ be a measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ two Markov kernels. If $f(1) = 0$ then $D_f(\kappa, \eta \mid \mu) \ge 0$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_nonneg}
Apply Lemma~\ref{lem:fDiv_nonneg}.
\end{proof}


\begin{lemma}
  \label{lem:condFDiv_const}
  \lean{ProbabilityTheory.condFDiv_const}
  \leanok
  \uses{def:condFDiv, def:fDiv}
  Let $\mu, \nu$ be measures on $\mathcal X$, where $\mu$ is finite, and let $\xi$ be a finite measure on $\mathcal Y$.
  Then $D_f(x \mapsto \mu, x \mapsto \nu \mid \xi) = D_f(\mu, \nu) \xi (\mathcal X)$.
\end{lemma}

\begin{proof}\leanok
\uses{}
$$D_f(x \mapsto \mu, x \mapsto \nu \mid \xi) 
= \xi\left[x \mapsto D_f(\mu, \nu)\right] 
= D_f(\mu, \nu) \xi\left[1\right] 
= D_f(\mu, \nu) \xi (\mathcal X)$$
\end{proof}


% TODO : this statement is not the same as the lean code, in particular here there is only one measure mentioned, while in the lean we have 2, also the rest of the statement is not the same
\begin{lemma}
  \label{lem:integrable_fDiv_compProd_iff}
  \lean{ProbabilityTheory.integrable_f_rnDeriv_compProd_iff}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels from $\mathcal X$ to $\mathcal Y$.
  Then $p \mapsto f \left(\frac{d(\mu \otimes \kappa)}{d(\mu \otimes \eta)}(p)\right)$ is $(\mu \otimes \eta)$-integrable iff
  \begin{itemize}
    \item $x \mapsto D_f(\kappa(x), \eta(x))$ is $\mu$-integrable and
    \item for $\mu$-almost all $x$, $y \mapsto f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right)$ is $\eta(x)$-integrable. 
  \end{itemize}
\end{lemma}

\begin{proof}
Since $x \mapsto f \left(\frac{d(\mu \otimes \kappa)}{d(\mu \otimes \eta)} x \right)$ is measurable, the integrability condition w.r.t. $\mu \otimes \eta$ is equivalent to
\begin{itemize}
    \item $x \mapsto \int_y \left\Vert f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \right\Vert \partial \eta(x)$ is $\mu$-integrable and
    \item for $\mu$-almost all $x$, $y \mapsto f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right)$ is $\eta(x)$-integrable. 
  \end{itemize}
It suffices to use convexity to show that integrability of $x \mapsto \int_y f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \partial \eta(x)$ implies integrability of $x \mapsto \int_y \left\Vert f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \right\Vert \partial \eta(x)$.

TODO
\end{proof}


\begin{lemma}
  \label{lem:condFDiv_ne_top_iff}
  \lean{ProbabilityTheory.condFDiv_ne_top_iff}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels, where $\kappa$ is a Markov kernel.
  Then $D_f(\kappa, \eta \mid \mu) \ne \infty$ if and only if
  \begin{itemize}
    \item for $\mu$-almost all $x$, $y \mapsto f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right)$ is $\eta(x)$-integrable,
    \item $x \mapsto \int_y f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \partial \eta(x)$ is $\mu$-integrable,
    \item either $f'(\infty) < \infty$ or for $\mu$-almost all $x$, $\kappa(x) \ll \eta(x)$.
  \end{itemize}
\end{lemma}

\begin{proof} \leanok
\uses{lem:integrable_fDiv_compProd_iff}
\end{proof}


\begin{lemma}
  \label{lem:fDiv_compProd_ne_top_iff}
  \lean{ProbabilityTheory.condFDiv_ne_top_iff_fDiv_compProd_ne_top}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels, such that $\kappa(x) \ne 0$ for all $x$.
  Then $D_f(\mu \otimes \kappa, \mu \otimes \eta) \ne \infty \iff D_f(\kappa, \eta \mid \mu) \ne \infty$.
\end{lemma}

\begin{proof} \leanok
\uses{lem:integrable_fDiv_compProd_iff, lem:condFDiv_ne_top_iff}
\end{proof}


\begin{lemma}
  \label{lem:fDiv_compProd_left}
  \lean{ProbabilityTheory.fDiv_compProd_left}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels, such that $\kappa(x) \ne 0$ for all $x$.
  Then $D_f(\mu \otimes \kappa, \mu \otimes \eta) = D_f(\kappa, \eta \mid \mu)$.
\end{lemma}

\begin{proof} \leanok
\uses{cor:rnDeriv_compProd_right, cor:rnDeriv_value, lem:fDiv_compProd_ne_top_iff, lem:condFDiv_ne_top_iff}
By Lemma~\ref{lem:fDiv_compProd_ne_top_iff}, the conditions on which the two divergences are finite are the same. We then assume integrability properties such that both are finite (given by Lemma~\ref{lem:condFDiv_ne_top_iff}).

TODO: the following proof assumes $\kappa(x) \ll \eta(x)$ for $\nu$-almost all $x$. Describe the general case.

By Lemma~\ref{cor:rnDeriv_compProd_right} and Corollary~\ref{cor:rnDeriv_value},
\begin{align*}
D_f(\mu \otimes \kappa, \mu \otimes \eta)
&= \int_{p} f\left(\frac{d (\mu \otimes \kappa)}{d (\mu \otimes \eta)}(p)\right) \partial(\mu \otimes \eta)
\\
&= \int_{p} f\left(\frac{d \kappa}{d \eta}(p)\right) \partial(\mu \otimes \eta)
\\
&= \int_x \int_y f\left(\frac{d \kappa}{d \eta}(x, y)\right) \partial \eta(x) \partial \mu
\\
&= \int_x \int_y f\left(\frac{d \kappa(x)}{d \eta(x)}(y)\right) \partial \eta(x) \partial \mu
\\
&= \mu\left[D_f(\kappa(x), \eta(x))\right]
= D_f(\kappa, \eta \mid \mu)
\: .
\end{align*}
\end{proof}




TODO: find a way to hide the following dummy lemma
\begin{lemma}[Dummy lemma: fDiv properties]
  \label{lem:fDiv_properties}
  %\lean{}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Dummy node to summarize properties of $f$-divergences.
\end{lemma}

\begin{proof}\leanok
\uses{
  lem:fDiv_ne_top_iff,
  lem:fDiv_const,
  lem:fDiv_self,
  lem:fDiv_eq_zero_iff,
  lem:fDiv_id,
  lem:fDiv_mul,
  lem:fDiv_add,
  lem:fDiv_add_linear,
  lem:fDiv_map_measurableEmbedding,
  lem:fDiv_symm,
  lem:fDiv_add_smul_left,
  lem:fDiv_add_smul_right,
  lem:fDiv_add_smul_right',
  lem:fDiv_absolutelyContinuous_add_mutuallySingular,
  lem:fDiv_add_measure_le,
  lem:fDiv_eq_add_withDensity_derivAtTop,
  lem:le_fDiv,
  lem:fDiv_nonneg,
  lem:condFDiv_nonneg,
  lem:condFDiv_const,
  lem:condFDiv_ne_top_iff,
  lem:fDiv_compProd_ne_top_iff,
  lem:fDiv_compProd_left
}
\end{proof}









\subsection{Integral representation of $f$-divergences}


\begin{definition}[Curvature measure]
  \label{def:curvatureMeasure}
  \lean{ConvexOn.curvatureMeasure}
  \leanok
  \uses{}
  Let $f: \mathbb{R} \to \mathbb{R}$ be a convex function. Then its right derivative $f'_+(x) \coloneqq \lim_{y \downarrow x}\frac{f(y) - f(x)}{y - x}$ is a Stieltjes function (a monotone right continuous function) and it defines a measure $\gamma_f$ on $\mathbb{R}$ by $\gamma_f((x,y]) \coloneqq f'_+(y) - f'_+(x)$~. \cite{liese2012phi} calls $\gamma_f$ the curvature measure of $f$.
\end{definition}


\begin{lemma}
  \label{lem:curvatureMeasure_mul}
  %\lean{}
  %\leanok
  \uses{def:curvatureMeasure}
  For $a \ge 0$ and $f: \mathbb{R} \to \mathbb{R}$ a convex function, the curvature measure of $af$ is $\gamma_{af} = a \gamma_f$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}


\begin{lemma}
  \label{lem:curvatureMeasure_add}
  %\lean{}
  %\leanok
  \uses{def:curvatureMeasure}
  For $f,g: \mathbb{R} \to \mathbb{R}$ two convex functions, the curvature measure of $f+g$ is $\gamma_{f+g} = \gamma_f + \gamma_g$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}


\begin{theorem}
  \label{thm:integration_by_parts}
  %\lean{}
  %\leanok
  \notready
  \uses{}
  If $f$ and $g$ are two Stieltjes functions with associated measures $\mu_f$ and $\mu_g$ and $f$ is continuous on $[a, b]$, then
  \begin{align*}
  \int_x f(x) \partial\mu_g = f(b)g(b) - f(a)g(a) - \int_x g(x) \partial\mu_f \: .
  \end{align*}
\end{theorem}

\begin{proof} \notready
\uses{}

\end{proof}


\begin{lemma}
  \label{lem:convex_taylor}
  \lean{ConvexOn.convex_taylor}
  \leanok
  \uses{def:curvatureMeasure}
  For $f: \mathbb{R} \to \mathbb{R}$ a convex function and $x,y \in \mathbb{R}$,
  \begin{align*}
  f(y) - f(x) - (y - x)f'_+(x) &= \int_{z \in (x,y]} (y - z) \partial \gamma_f & \text{ if } x \le y \: ,
  \\
  f(y) - f(x) - (y - x)f'_+(x) &= \int_{z \in (y,x]} (z - y) \partial \gamma_f & \text{ if } y \le x \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{thm:integration_by_parts}
Let $\Lambda$ be the Lebesgue measure and let $x < y$. Since $f$ has right derivative $f'_+$ in $(x,y)$ and $f'_+$ is integrable on that interval,
\begin{align*}
f(y) - f(x) = \int_{z \in (x, y]} f'_+(z) \partial \Lambda
\: .
\end{align*}
We now integrate by parts, using Theorem~\ref{thm:integration_by_parts}. $\Lambda$ is the measure obtained from the Stieltjes function $z \mapsto z - y$.
\begin{align*}
\int_{z \in (x, y]} f'_+(z) \partial \Lambda
&= - \int_{z \in (x,y]} (z - y)\partial \gamma_f + f'_+(y)(y - y) - f'_+(x)(x - y)
\\
&= \int_{z \in (x,y]} (y - z)\partial \gamma_f + f'_+(x)(y - x)
\: .
\end{align*}
Putting the two equalities together, we get the result for $x < y$. The proof for $x > y$ is similar, and for $x = y$ both sides of both equations are zero.
\end{proof}


\begin{definition}
  \label{def:statInfoFun}
  \lean{ProbabilityTheory.statInfoFun}
  \leanok
  \uses{}
  For $a,b \in (0, +\infty)$ let $\phi_{a,b} : \mathbb{R} \to \mathbb{R}$ be the function defined by
  \begin{align*}
  \phi_{a,b}(x) &= \max\left\{0, a x - b \right\} & \text{ for } a \le b \: ,
  \\
  \phi_{a,b}(x) &= \max\left\{0, b - a x \right\} & \text{ for } a > b \: .
  \end{align*}
\end{definition}


\begin{corollary}
  \label{cor:convex_taylor_statInfoFun}
  \lean{ProbabilityTheory.integral_statInfoFun_curvatureMeasure}
  \leanok
  \uses{def:curvatureMeasure, def:statInfoFun}
  For $f: \mathbb{R} \to \mathbb{R}$ a convex function, for all $x \in \mathbb{R}$~,
  \begin{align*}
  f(x) &= f(1) + f'_+(1) (x - 1) + \int_{y} \phi_{1,y}(x) \partial\gamma_f \: .
  \end{align*}
\end{corollary}

\begin{proof}\leanok
\uses{lem:convex_taylor}
By Lemma~\ref{lem:convex_taylor}, for $x > 1$,
\begin{align*}
f(x) - f(1) - f'_+(1) (x - 1)
&= \int_{y \in (1, x]} (x - y) \partial\gamma_f
\\
&= \int_{y \in (1, +\infty)} \max\{0, x - y\} \partial\gamma_f
\\
&= \int_{y \in (1, +\infty)} \phi_{1,y}(x) \partial\gamma_f
\: .
\end{align*}
That final integral is equal to $\int_y \phi_{1,y}(x) \partial\gamma_f$ (without the restriction to $(1, +\infty)$) because $\phi_{1,y}(x)$ is zero outside of $(1, +\infty)$ for $x \ge 1$.

The case of $x < 1$ is similar.
\end{proof}


\begin{lemma}
  \label{lem:curvatureMeasure_statInfoFun}
  %\lean{}
  %\leanok
  \uses{def:curvatureMeasure, def:statInfoFun}
  The curvature measure of the function $\phi_{a,b}$ is $\gamma_{\phi_{a,b}} = a\delta_{b/a}$~, where $\delta_x$ is the Dirac measure at $x$.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}

The following theorem is inspired from \cite{liese2006divergences,liese2012phi} but gives a different integral representation.

\begin{theorem}
  \label{thm:fDiv_eq_integral_eGamma}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:curvatureMeasure, def:eGamma}
  For two finite measures $\mu, \nu \in \mathcal M(\mathcal X)$,
  \begin{align*}
  D_f(\mu, \nu) = f(1) \nu(\mathcal X) + f'_+(1)(\mu(\mathcal X) - \nu(\mathcal X)) + \int_y D_{\phi_{1,y}}(\mu, \nu) \partial\gamma_f \: .
  \end{align*}
\end{theorem}

For probability measures, $D_{\phi_{1,y}}(\mu, \nu) = E_y(\mu, \nu)$~, but these functions can differ if $\mu$ and $\nu$ are finite measures with different masses.

\begin{proof}%\leanok
\uses{cor:convex_taylor_statInfoFun}
We prove the result for $f(1) = 0$ and $f'_+(1) = 0$ for simplicity of exposition.

From Corollary~\ref{cor:convex_taylor_statInfoFun} we get that $f(x) = \int_y \phi_{1,y}(x) \partial\gamma_f$~.
From this and the theorem of Fubini,
\begin{align*}
\int_x f\left( \frac{d \mu}{d \nu}(x) \right) \partial\nu
= \int_{x} \int_{y} \phi_{1,y} \left(\frac{d \mu}{d \nu}(x) \right) \partial\gamma_f \partial\nu
= \int_{y} \int_{x} \phi_{1,y} \left(\frac{d \mu}{d \nu}(x) \right) \partial\nu \partial\gamma_f
\: .
\end{align*}
For the mutually singular part of the divergence $D_f$,
\begin{align*}
f'(\infty)
= f'_+(\infty) - f'_+(1) 
= \gamma_f((1,+\infty))
= \int_{y > 1} 1 \partial \gamma_f
= \int_{y > 1} \phi_{1,y}'(\infty) \partial \gamma_f
\: .
\end{align*}
The last integral is equal to $\int_{y \in \mathbb{R}} \phi_{1,y}'(\infty) \partial \gamma_f$ since $\phi_{1,y}'(\infty) = 0$ for $y \le 1$.
Finally,
\begin{align*}
D_f(\mu, \nu)
&= \int_x f\left( \frac{d \mu}{d \nu}(x) \right) \partial\nu + f'(\infty) \mu_{\perp \nu}(\mathcal X)
\\
&= \int_{y} \left(\int_{x} \phi_{1,y} \left(\frac{d \mu}{d \nu}(x) \right) \partial\nu + \phi_{1,y}'(\infty) \mu(\mathcal X) \right) \partial\gamma_f
\\
&= \int_y D_{\phi_{1,y}}(\mu, \nu) \partial \gamma_f
\: .
\end{align*}
\end{proof}

TODO: define $\Gamma_f$.

TODO: in the lemma below, $I_\pi(\mu, \nu)$ is the correct value for probability measures, but in general it should be replaced by $D_{\phi_{\pi, 1 - \pi}}(\mu, \nu)$~.
These functions can differ if $\mu$ and $\nu$ are finite measures with different masses.

\begin{theorem}[\cite{liese2006divergences,liese2012phi}]
  \label{thm:fDiv_eq_integral}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:curvatureMeasure, def:deGrootInfo}
  \begin{align*}
  D_f(\mu, \nu) = f(1) \nu(\mathcal X) + f'_+(1)(\mu(\mathcal X) - \nu(\mathcal X)) + \int_\pi I_\pi(\mu, \nu) \partial\Gamma_f \: .
  \end{align*}
  
\end{theorem}

\begin{proof}%\leanok
\uses{thm:fDiv_eq_integral_eGamma}

\end{proof}




\subsection{Data-processing inequality}

This section contains three proofs of the data-processing inequality for $f$-divergences.
That inequality states that for $\mu, \nu$ two measures on $\mathcal X$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu, \nu)$.
\begin{enumerate}
  \item A first proof deals only with measurable functions (or deterministic kernels): in that case the inequality is equivalent to an inequality about restrictions of the measures to sub-$\sigma$-algebras.
  \item A second proof deals with the general case and uses Radon-Nikodym derivatives of kernels. This restrict its applicability to measurable spaces that satisfy some particular countability assumptions.
  \item The last proof uses the representation of $f$-divergence as an integral over statistical (risk-based) divergences, then uses the data-processing inequality for those divergences.
\end{enumerate}



\subsubsection{Sigma-algebras}

For $\mathcal A$ a sub-$\sigma$-algebra and $\mu$ a measure, we write $\mathcal \mu_{| \mathcal A}$ for the measure restricted to the $\sigma$-algebra.

\begin{lemma}
  \label{lem:rnDeriv_trim_of_ac}
  \lean{MeasureTheory.Measure.toReal_rnDeriv_trim_of_ac}
  \leanok
  %\uses{}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ with $\mu \ll \nu$ and let $\mathcal A$ be a sub-$\sigma$-algebra of $\mathcal X$.
  Then $\frac{d \mu_{| \mathcal A}}{d \nu_{| \mathcal A}}$ is $\nu_{| \mathcal A}$-almost everywhere (hence also $\nu$-a.e.) equal to $\nu\left[ \frac{d \mu}{d \nu} \mid \mathcal A\right]$.
\end{lemma}

\begin{proof}\leanok
\end{proof}

\begin{lemma}[Superseded by Theorem~\ref{thm:fDiv_trim_le}]
  \label{lem:fDiv_trim_le_of_ac}
  \lean{ProbabilityTheory.fDiv_trim_le_of_ac}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ with $\mu \ll \nu$ and let $\mathcal A$ be a sub-$\sigma$-algebra of $\mathcal X$. Then
  $D_f(\mu_{| \mathcal A}, \nu_{| \mathcal A}) \le D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{thm:condexp_jensen, lem:rnDeriv_trim_of_ac}
Since $\mu \ll \nu$, $\mu_{| \mathcal A} \ll \nu_{| \mathcal A}$ and
\begin{align*}
D_f(\mu_{| \mathcal A}, \nu_{| \mathcal A})
&= \int_x f \left( \frac{d \mu_{| \mathcal A}}{d \nu_{| \mathcal A}}(x) \right) \partial\nu_{| \mathcal A} \: .
\end{align*}
Since $f \left( \frac{d \mu_{| \mathcal A}}{d \nu_{| \mathcal A}} \right)$ is $\mathcal A$-measurable, this integral is equal to the integral w.r.t. $\nu$.
By Lemma~\ref{lem:rnDeriv_trim_of_ac}, this is equal to $\int_x f \left( \nu\left[ \frac{d \mu}{d \nu} \mid \mathcal A\right] (x) \right) \partial\nu$.

By Theorem~\ref{thm:condexp_jensen}, $f \left( \nu\left[ \frac{d \mu}{d \nu} \mid \mathcal A\right] (x) \right) \le \nu\left[ f \circ \frac{d \mu}{d \nu} \mid \mathcal A\right] (x)$ almost everywhere.
Finally
\begin{align*}
D_f(\mu_{| \mathcal A}, \nu_{| \mathcal A})
&= \int_x f \left( \nu\left[ \frac{d \mu}{d \nu} \mid \mathcal A\right] (x) \right) \partial\nu
\\
&\le \int_x \nu\left[ f \circ \frac{d \mu}{d \nu} \mid \mathcal A\right] (x) \partial\nu
\\
&= \int_x f \left( \frac{d \mu}{d \nu} (x) \right) \partial\nu
\\
&= D_f(\mu, \nu)
\: .
\end{align*}

\end{proof}

\begin{theorem}
  \label{thm:fDiv_trim_le}
  \lean{ProbabilityTheory.fDiv_trim_le}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\mathcal A$ be a sub-$\sigma$-algebra of $\mathcal X$. Then
  $D_f(\mu_{| \mathcal A}, \nu_{| \mathcal A}) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}\leanok
\uses{lem:fDiv_trim_le_of_ac, lem:fDiv_add_measure_le}
We decompose $\mu_{| \mathcal A}$ into two parts: $\mu_{| \mathcal A} = (\frac{d\mu}{d\nu}\cdot \nu)_{| \mathcal A} + (\mu_{\perp \nu})_{| \mathcal A}$~. 
We have $(\frac{d\mu}{d\nu}\cdot \nu)_{| \mathcal A} \ll \nu_{| \mathcal A}$.
By Lemma~\ref{lem:fDiv_add_measure_le},
\begin{align*}
D_f(\mu_{| \mathcal A}, \nu_{| \mathcal A})
\le D_f\left((\frac{d\mu}{d\nu}\cdot \nu)_{| \mathcal A}, \nu_{| \mathcal A}\right)
  + (\mu_{\perp \nu})_{| \mathcal A}(\mathcal X) f'(\infty)
\: .
\end{align*}
Then we apply Lemma~\ref{lem:fDiv_trim_le_of_ac} to the first term to get
\begin{align*}
D_f(\mu_{| \mathcal A}, \nu_{| \mathcal A})
&\le D_f\left(\frac{d\mu}{d\nu}\cdot \nu, \nu\right)
  + (\mu_{\perp \nu})_{| \mathcal A}(\mathcal X) f'(\infty)
\\
&= D_f\left(\frac{d\mu}{d\nu}\cdot \nu, \nu\right)
  + \mu_{\perp \nu}(\mathcal X) f'(\infty)
\\
&= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}


\begin{theorem}
  \label{thm:iSup_fDiv_trim}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$. Then
  $\sup_{\mathcal A \text{ finite}} D_f(\mu_{| \mathcal A}, \nu_{| \mathcal A}) = D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_trim_le}
\end{proof}


We now show how the data-processing inequality on $\sigma$-algebras implies a data-processing inequality for composition with measurable functions.

\begin{lemma}
  \label{lem:rnDeriv_map_eq_condexp}
  %\lean{}
  %\leanok
  \uses{}
  Let $\mu, \nu \in \mathcal M(\mathcal X)$, $g : \mathcal X \to \mathcal Y$ a measurable function and denote by $g^* \mathcal Y$ the comap of the $\sigma$-algebra on $\mathcal Y$ by $g$.
  Then $\nu$-almost everywhere,
  \begin{align*}
  \frac{d g_*\mu}{d g_*\nu}(g(x)) = \nu\left[ \frac{d \mu}{d \nu} \mid g^* \mathcal Y\right](x)
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}


\begin{lemma}
  \label{lem:rnDeriv_map_eq_rnDeriv_trim}
  %\lean{}
  %\leanok
  \uses{}
  Let $\mu, \nu \in \mathcal M(\mathcal X)$ with $\mu \ll \nu$, $g : \mathcal X \to \mathcal Y$ a measurable function and denote by $g^* \mathcal Y$ the comap of the $\sigma$-algebra on $\mathcal Y$ by $g$.
  Then $\nu$-almost everywhere,
  \begin{align*}
  \frac{d g_*\mu}{d g_*\nu}(g(x)) = \frac{d \mu_{| g^* \mathcal Y}}{d \nu_{| g^* \mathcal Y}}(x)
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:rnDeriv_map_eq_condexp, lem:rnDeriv_trim_of_ac}

\end{proof}


\begin{lemma}
  \label{lem:fDiv_map_eq_fDiv_trim_of_ac}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu \in \mathcal M(\mathcal X)$ be finite measures with $\mu \ll \nu$ and let $g : \mathcal X \to \mathcal Y$ be a measurable function. Denote by $g^* \mathcal Y$ the comap of the $\sigma$-algebra on $\mathcal Y$ by $g$. Then
  $D_f(g_* \mu, g_* \nu) = D_f(\mu_{| g^* \mathcal Y}, \nu_{| g^* \mathcal Y})$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:rnDeriv_map_eq_condexp}
\begin{align*}
D_f(g_* \mu, g_* \nu)
&= \int_y f \left( \frac{d g_*\mu}{d g_*\nu}(y)\right) \partial(g_*\nu)
\\
&= \int_x f \left( \frac{d g_*\mu}{d g_*\nu}(g(x))\right) \partial\nu
\\
&= \int_x f \left(\frac{d \mu_{| g^* \mathcal Y}}{d \nu_{| g^* \mathcal Y}}(x) \right) \partial\nu
\\
&= \int_x f \left(\frac{d \mu_{| g^* \mathcal Y}}{d \nu_{| g^* \mathcal Y}}(x) \right) \partial\nu_{| g^* \mathcal Y}
\\
&= D_f(\mu_{| g^* \mathcal Y}, \nu_{| g^* \mathcal Y})
\: .
\end{align*}
\end{proof}


\begin{lemma}[Superseded by Theorem~\ref{thm:fDiv_map_le}]
  \label{lem:fDiv_map_le_of_ac}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ with $\mu \ll \nu$ and let $g : \mathcal X \to \mathcal Y$ be a measurable function. Then
  $D_f(g_* \mu, g_* \nu) \le D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:fDiv_map_eq_fDiv_trim_of_ac, thm:fDiv_trim_le}
By Lemma~\ref{lem:fDiv_map_eq_fDiv_trim_of_ac},  $D_f(g_* \mu, g_* \nu) = D_f(\mu_{| g^* \mathcal Y}, \nu_{| g^* \mathcal Y})$. Then apply Theorem~\ref{thm:fDiv_trim_le}.
\end{proof}


\begin{theorem}
  \label{thm:fDiv_map_le}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $g : \mathcal X \to \mathcal Y$ be a measurable function. Then
  $D_f(g_* \mu, g_* \nu) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}%\leanok
\uses{lem:fDiv_map_le_of_ac, lem:fDiv_add_measure_le}
We use $g_*\mu = g_*(\frac{d\mu}{d\nu}\cdot \nu) + g_*\mu_{\perp \nu}$~. 
We have $g_*(\frac{d\mu}{d\nu}\cdot \nu) \ll g_*\nu$.
By Lemma~\ref{lem:fDiv_add_measure_le},
\begin{align*}
D_f(g_* \mu, g_* \nu)
\le D_f\left(g_*(\frac{d\mu}{d\nu}\cdot \nu), g_*\nu\right)
  + g_*\mu_{\perp \nu}(\mathcal Y) f'(\infty)
\: .
\end{align*}
Then we apply Lemma~\ref{lem:fDiv_map_le_of_ac} to the first term to get
\begin{align*}
D_f(g_* \mu, g_* \nu)
&\le D_f\left(\frac{d\mu}{d\nu}\cdot \nu, \nu\right)
  + g_*\mu_{\perp \nu}(\mathcal Y) f'(\infty)
\\
&= D_f\left(\frac{d\mu}{d\nu}\cdot \nu, \nu\right)
  + \mu_{\perp \nu}(\mathcal X) f'(\infty)
\\
&= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}


\subsubsection{Kernel proof}

This proof starts by establishing $D_f(\mu, \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$ for Markov kernels $\kappa$ and $\eta$, then deduces the DPI.
Indeed, if the measurable spaces are standard Borel, we can write $\mu \otimes \kappa = ((\kappa \circ \mu) \otimes \kappa^\dagger_\mu)_\leftrightarrow$ for a Markov kernel $\kappa^\dagger_\mu$ (see Definition~\ref{def:bayesInv}; $(...)_\leftrightarrow$ is notation for composition with the map that swaps the coordinates) and then use the inequality to get
\begin{align*}
D(\kappa \circ \mu, \kappa \circ \nu)
&\le D((\kappa \circ \mu) \otimes \kappa^\dagger_\mu, (\kappa \circ \nu) \otimes \kappa^\dagger_\nu)
\\
&= D(((\kappa \circ \mu) \otimes \kappa^\dagger_\mu)_{\leftrightarrow}, ((\kappa \circ \nu) \otimes \kappa^\dagger_\nu)_{\leftrightarrow})
\\
&= D(\mu \otimes \kappa, \nu \otimes \kappa)
\\
&= D(\mu, \nu)
\: .
\end{align*}
The main drawback of this approach is that it requires that the Bayesian inverse exists, which is not always the case, unless for example the measurable spaces are standard Borel.

The inequality $D_f(\mu, \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$ is obtained by describing the Radon-Nikodym derivative $\frac{d(\mu \otimes \kappa)}{d(\nu \otimes \eta)}$ and using convexity properties of the function $f$.

\begin{theorem}
  \label{thm:fDiv_le_compProd_1}
  \lean{ProbabilityTheory.le_fDiv_compProd}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  Then $D_f(\mu, \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$.
\end{theorem}

\begin{proof}\leanok
\uses{lem:rnDeriv_compProd, cor:rnDeriv_value}
TODO: the following proof assumes $\mu \ll \nu$ and $\kappa(x) \ll \eta(x)$ for $\nu$-almost all $x$. Describe the general case.

Using Lemma~\ref{lem:rnDeriv_compProd} and \ref{cor:rnDeriv_value},
\begin{align*}
D(\mu \otimes \kappa, \nu \otimes \eta)
&= \int_x \int_y f \left( \frac{\partial \mu}{\partial\nu}(x) \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \right) \partial \eta(x) \partial \nu
\: .
\end{align*}
Since $f$ is convex, by Jensen's inequality,
\begin{align*}
\int_y f \left( \frac{\partial \mu}{\partial\nu}(x) \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \right) \partial \eta(x)
&\ge f \left( \int_y \frac{\partial \mu}{\partial\nu}(x) \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \partial \eta(x) \right)
\\
&= f \left( \frac{\partial \mu}{\partial\nu}(x) \int_y \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \partial \eta(x) \right)
\: .
\end{align*}

Since $\kappa(x) \ll \eta(x)$ for $\nu$-almost all $x$, $\int_y \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \partial \eta(x) = \int_y 1 \partial \kappa(x) = 1$ a.e.. We have obtained
\begin{align*}
D_f(\mu \otimes \kappa, \nu \otimes \eta)
\ge \int_x f \left( \frac{\partial \mu}{\partial\nu}(x)\right) \partial \nu
= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}

\begin{theorem}[Marginals]
  \label{thm:fDiv_fst_le_1}
  \lean{ProbabilityTheory.fDiv_fst_le}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two measures on $\mathcal X \times \mathcal Y$ where $\mathcal Y$ is standard Borel, and let $\mu_X, \nu_X$ be their marginals on $\mathcal X$.
  Then $D_f(\mu_X, \nu_X) \le D_f(\mu, \nu)$.
  Similarly, for $\mathcal X$ standard Borel and $\mu_Y, \nu_Y$ the marginals on $\mathcal Y$, $D_f(\mu_Y, \nu_Y) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}\leanok
\uses{thm:fDiv_le_compProd_1, lem:fDiv_map_measurableEmbedding}
We introduce conditional kernels and write $D(\mu, \nu) = D(\mu_X \otimes \mu_{Y|X}, \nu_X \otimes \nu_{Y|X})$. Then apply Theorem~\ref{thm:fDiv_le_compProd_1}.
For the second statement, we need Lemma~\ref{lem:fDiv_map_measurableEmbedding} to swap the role of the two coordinates.
\end{proof}

\begin{lemma}[Composition-product with a kernel]
  \label{thm:fDiv_compProd_right_1}
  \lean{ProbabilityTheory.fDiv_compProd_right}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $D_f(\mu \otimes \kappa, \nu \otimes \kappa) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{cor:rnDeriv_compProd_left}
By Corollary~\ref{cor:rnDeriv_compProd_left},
\begin{align*}
D_f(\mu \otimes \kappa, \nu \otimes \kappa)
&= \int_{p} f\left(\frac{d (\mu \otimes \kappa)}{d (\nu \otimes \kappa)}(p)\right) \partial(\nu \otimes \kappa)
\\
&= \int_{p} f\left(\frac{d \mu}{d \nu}(p_X)\right) \partial(\nu \otimes \kappa)
\\
&= \int_x \int_y f\left(\frac{d \mu}{d \nu}(x)\right) \partial \kappa(x) \partial \nu
\\
&= \int_x f\left(\frac{d \mu}{d \nu}(x)\right) \partial \nu
\\
&= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}

\begin{corollary}
  \label{cor:fDiv_prod_right_1}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\xi$ be a measure on $\mathcal Y$.
  Then $D_f(\mu \times \xi, \nu \times \xi) = D_f(\mu, \nu)$.
\end{corollary}

\begin{proof}
\uses{thm:fDiv_compProd_right_1}
Apply Lemma~\ref{thm:fDiv_compProd_right_1} with $\kappa$ the constant kernel with value $\xi$.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_comp_le_compProd_1}
  \lean{ProbabilityTheory.fDiv_comp_le_compProd}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on a standard Borel space $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels.
  $D_f(\kappa \circ \mu, \eta \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$
\end{lemma}

\begin{proof}\leanok
\uses{thm:fDiv_fst_le_1}
By definition, $\kappa \circ \mu$ is the marginal of $\mu \otimes \kappa$ (a measure on $\mathcal X \times \mathcal Y$) on $\mathcal Y$, and similarly for the other measure. Hence by Theorem~\ref{thm:fDiv_fst_le_1}, $D_f(\kappa \circ \mu, \eta \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$. 
\end{proof}

\begin{theorem}[Conditioning increases f-divergence]
  \label{thm:fDiv_comp_le_condFDiv_1}
  \lean{ProbabilityTheory.fDiv_comp_left_le}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a measure on a standard Borel space $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels, such that $\kappa(x) \ne 0$ for all $x$.
  Then $D_f(\kappa \circ \mu, \eta \circ \mu) \le D_f(\mu \otimes \kappa, \mu \otimes \eta) = D_f(\kappa, \eta \mid \mu)$
\end{theorem}

\begin{proof}\leanok
\uses{lem:fDiv_comp_le_compProd_1, lem:fDiv_compProd_left}
By Lemma~\ref{lem:fDiv_comp_le_compProd_1}, $D_f(\kappa \circ \mu, \eta \circ \mu) \le D_f(\mu \otimes \kappa, \mu \otimes \eta)$. This is equal to $D_f(\kappa, \eta \mid \mu)$ by Lemma~\ref{lem:fDiv_compProd_left}.
\end{proof}

\begin{theorem}[Data-processing]
  \label{thm:fDiv_data_proc_1}
  \lean{ProbabilityTheory.fDiv_comp_right_le}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel, where both $\mathcal X$ and $\mathcal Y$ are standard Borel.
  Then $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}\leanok
\uses{lem:fDiv_comp_le_compProd_1, thm:fDiv_compProd_right_1}
By Lemma~\ref{lem:fDiv_comp_le_compProd_1}, $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \kappa)$. Then the latter is equal to $D_f(\mu, \nu)$ by Lemma~\ref{thm:fDiv_compProd_right_1}.
\end{proof}


\begin{corollary}
  \label{cor:data_proc_event}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event. Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$. Then $D_f(\mu, \nu) \ge D_f(\mu_E, \nu_E)$.
\end{corollary}

\begin{proof}
\uses{thm:fDiv_data_proc_1}
Use the deterministic kernel $\kappa : \mathcal X \rightsquigarrow \{0, 1\}$ with $\kappa(x) = \delta_1 \mathbb{I}\{x \in E\} + \delta_0 \mathbb{I}\{x \notin E\}$ in Theorem~\ref{thm:fDiv_data_proc_1}.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_compProd_prod_eq}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow (\mathcal X \times \mathcal Y)$ be a Markov kernel such that for all $x$, $(\kappa(x))_X = \delta_x$. Then $D_f(\kappa \circ \mu, \kappa \circ \nu) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}
\uses{thm:fDiv_data_proc_1, thm:fDiv_fst_le_1}
$D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu, \nu)$ by Theorem~\ref{thm:fDiv_data_proc_1}.
For the other inequality, remark that $\mu = (\kappa \circ \mu)_X$ (and similarly for $\nu$). Hence by Theorem~\ref{thm:fDiv_fst_le_1} $D_f(\mu, \nu) \le D_f(\kappa \circ \mu, \kappa \circ \nu)$.
\end{proof}




\subsubsection{General measurable spaces}

This proof uses that every $f$-divergence is an integral over $\gamma \ge 0$ of $f$-divergences for the function $\phi_{1,\gamma}$ of Definition~\ref{def:statInfoFun}. It suffices then to prove the DPI for those particular $f$-divergences. Those are equal to the $E_\gamma$ divergence, up to a term that is invariant by composition with a Markov kernel. Finally, we already know that the $E_\gamma$ divergence satisfies the DPI, and that completes the proof.

\begin{lemma}
  \label{lem:fDiv_statInfoFun_eq}
  \lean{ProbabilityTheory.fDiv_statInfoFun_eq_StatInfo_of_nonneg_of_le, ProbabilityTheory.fDiv_statInfoFun_eq_StatInfo_of_nonneg_of_gt}
  \leanok
  \uses{def:fDiv, def:statInfoFun}
  Let $a,b \in [0, +\infty)$ and let $\mu, \nu$ be two measures on $\mathcal X$.
  \begin{align*}
  D_{\phi_{a,b}}(\mu, \nu)
  &= \text{sign}(b-a)\frac{1}{2}(a \mu(\mathcal X) - b \nu(\mathcal X)) + \frac{1}{2}\nu\left[ \left\vert a \frac{d\mu}{d\nu} - b \right\vert \right] + \frac{1}{2}a \mu_{\perp \nu}(\mathcal X)
  \: ,
  \end{align*}
  in which $\text{sign}(b-a)$ is $1$ if $b-a > 0$ and $-1$ if $b-a \le 0$.
\end{lemma}

\begin{proof}\leanok
\uses{}
If $a \le b$,
\begin{align*}
D_{\phi_{a,b}}(\mu, \nu)
&= \nu\left[ \max\{0, a \frac{d\mu}{d\nu} - b\} \right] + a \mu_{\perp \nu}(\mathcal X)
\\
&= \frac{1}{2}(a \mu_{\parallel \nu}(\mathcal X) - b \nu(\mathcal X)) + \frac{1}{2}\nu\left[ \left\vert a \frac{d\mu}{d\nu} - b \right\vert \right] + a \mu_{\perp \nu}(\mathcal X)
\\
&= \frac{1}{2}(a \mu(\mathcal X) - b \nu(\mathcal X)) + \frac{1}{2}\nu\left[ \left\vert a \frac{d\mu}{d\nu} - b \right\vert \right] + \frac{1}{2}a \mu_{\perp \nu}(\mathcal X)
\: . 
\end{align*}
Similar computations lead to the result for $b \le a$.
\end{proof}

\begin{corollary}
  \label{cor:fDiv_statInfoFun_eq_statInfo}
  \lean{ProbabilityTheory.fDiv_statInfoFun_eq_StatInfo_of_nonneg}
  \leanok
  \uses{def:fDiv, def:statInfoFun, def:statInfo}
  Let $a,b \in [0, +\infty)$ and let $\mu, \nu$ be two measures on $\mathcal X$.
  \begin{align*}
  D_{\phi_{a,b}}(\mu, \nu) = \mathcal I_{(a,b)}(\mu, \nu) + \frac{1}{2} \left\vert a \mu(\mathcal X) - b \nu(\mathcal X) \right\vert + \text{sign}(b-a)\frac{1}{2}(a \mu(\mathcal X) - b \nu(\mathcal X))
  \: .
  \end{align*}
\end{corollary}

\begin{proof}\leanok
\uses{lem:fDiv_statInfoFun_eq, cor:statInfo_eq_integral_abs}
Combine Lemma~\ref{lem:fDiv_statInfoFun_eq} and Corollary~\ref{cor:statInfo_eq_integral_abs}.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_phi_data_proc}
  \lean{ProbabilityTheory.fDiv_statInfoFun_comp_right_le}
  \leanok
  \uses{def:fDiv, def:statInfoFun}
  Let $a,b \in [0, +\infty)$. Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $D_{\phi_{a,b}}(\kappa \circ \mu, \kappa \circ \nu) \le D_{\phi_{a,b}}(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{thm:data_proc_statInfo, cor:fDiv_statInfoFun_eq_statInfo}
By Corollary~\ref{cor:fDiv_statInfoFun_eq_statInfo},
\begin{align*}
D_{\phi_{a,b}}(\mu, \nu) = \mathcal I_{(a,b)}(\mu, \nu) + \frac{1}{2} \left\vert a \mu(\mathcal X) - b \nu(\mathcal X) \right\vert + \text{sign}(b-a)\frac{1}{2}(a \mu(\mathcal X) - b \nu(\mathcal X))
\: .
\end{align*}
The statistical information satisfies the data-processing inequality (Theorem~\ref{thm:data_proc_statInfo}) and the other terms are invariant by composition with a Markov kernel, hence $D_{\phi_{a,b}}$ also satisfies the data-processing inequality.
\end{proof}

\begin{theorem}[Data-processing]
  \label{thm:fDiv_data_proc_2}
  \lean{ProbabilityTheory.fDiv_comp_right_le'}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}\leanok
\uses{thm:fDiv_eq_integral_eGamma, lem:fDiv_phi_data_proc}
By Theorem~\ref{thm:fDiv_eq_integral_eGamma},
\begin{align*}
D_f(\mu, \nu) = f(1) \nu(\mathcal X) + f'_+(1) (\mu(\mathcal X) - \nu (\mathcal X)) + \int_y D_{\phi_{1,y}}(\mu, \nu) \partial\gamma_f \: .
\end{align*}
The first two terms are unchanged by composition with a Markov kernel. It thus suffices to prove the data processing inequality for the divergences $D_{\phi_{1, \gamma}}$. This was done in Lemma~\ref{lem:fDiv_phi_data_proc}.
\end{proof}


\begin{theorem}
  \label{thm:fDiv_le_compProd_2}
  \lean{ProbabilityTheory.le_fDiv_compProd'}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  Then $D_f(\mu, \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$.
\end{theorem}

\begin{proof}\leanok
\uses{thm:fDiv_data_proc_2}
Since $\kappa$ is a Markov kernel, $\mu$ is the composition of $\mu \otimes \kappa$ and the deterministic kernel for the function $(x,y) \mapsto x$. The same applies for $\nu$ and $\nu \otimes \eta$.
The result is then an application of the data-processing inequality, Theorem~\ref{thm:fDiv_data_proc_2}.
\end{proof}


\begin{lemma}[Composition-product with a kernel]
  \label{thm:fDiv_compProd_right_2}
  \lean{ProbabilityTheory.fDiv_compProd_right'}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $D_f(\mu \otimes \kappa, \nu \otimes \kappa) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{thm:fDiv_le_compProd_2, thm:fDiv_data_proc_2}
By Theorem~\ref{thm:fDiv_le_compProd_2}, $D_f(\mu, \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \kappa)$.
For the reverse inequality, remark that $\mu \otimes \kappa = (\text{id} \times \kappa) \circ \mu$ where $\text{id}$ is the identity kernel and apply the data-processing inequality (Theorem~\ref{thm:fDiv_data_proc_2}). 
\end{proof}

\begin{theorem}
  \label{thm:fDiv_comp_le_compProd_2}
  \lean{ProbabilityTheory.fDiv_comp_le_compProd'}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels.
  Then $D_f(\kappa \circ \mu, \eta \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$.
\end{theorem}

\begin{proof}\leanok
\uses{thm:fDiv_data_proc_2}
$\kappa \circ \mu$ is the composition of $\mu \otimes \kappa$ and the deterministic kernel for the function $(x,y) \mapsto y$. The same applies for $\eta \circ \nu$ and $\nu \otimes \eta$.
The result is then an application of the data-processing inequality, Theorem~\ref{thm:fDiv_data_proc_2}.
\end{proof}


\begin{theorem}[Conditioning increases f-divergence]
  \label{thm:fDiv_comp_le_condFDiv_2}
  \lean{ProbabilityTheory.fDiv_comp_le_compProd_right'}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a finite measure $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels.
  Then $D_f(\kappa \circ \mu, \eta \circ \mu) \le D_f(\mu \otimes \kappa, \mu \otimes \eta)$~.
\end{theorem}

\begin{proof}\leanok
\uses{thm:fDiv_comp_le_compProd_2}
This is a particular case of Theorem~\ref{thm:fDiv_comp_le_compProd_2}.
\end{proof}


\begin{theorem}[Marginals]
  \label{thm:fDiv_fst_le_2}
  \lean{ProbabilityTheory.fDiv_fst_le', ProbabilityTheory.fDiv_snd_le'}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two measures on $\mathcal X \times \mathcal Y$, and let $\mu_X, \nu_X$ be their marginals on $\mathcal X$.
  Then $D_f(\mu_X, \nu_X) \le D_f(\mu, \nu)$.
  Similarly, for $\mu_Y, \nu_Y$ the marginals on $\mathcal Y$, $D_f(\mu_Y, \nu_Y) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}\leanok
\uses{thm:fDiv_data_proc_2}
The measure $\mu_X$ is the composition of $\mu$ and the deterministic kernel for the function $(x,y) \mapsto x$, and similarly $\mu_Y$ is the composition of $\mu$ and a deterministic kernel.
The results are both applications of the data-processing inequality, Theorem~\ref{thm:fDiv_data_proc_2}.
\end{proof}




\subsection{Data-processing and mean values}

In this section $d_f(p, q)$ denotes the $f$-divergence between two Bernoulli distributions with means $p$ and $q$.


\begin{lemma}
  \label{lem:fDiv_bernoulli_convex}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  For all $y \in [0,1]$, $x \mapsto d_f(x, y)$ is convex and attains a minimum at $x = y$.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\begin{align*}
d_f(x, y)
&= y f(\frac{x}{y}) + (1 - y) f(\frac{1 - x}{1 - y})
\: , \\
(x \mapsto d_f(x, y))'_+(x)
&= f'_+(\frac{x}{y}) - f'_+(\frac{1 - x}{1 - y})
\: .
\end{align*}
$f'_+$ is non-decreasing, hence we get that $(x \mapsto d_f(x, y))'_+$ is also non-decreasing and $x \mapsto d_f(x, y)$ is convex.

For $x \le y$, $x/y \le 1 \le (1-x)/(1-y)$, hence $f'_+(\frac{x}{y}) \le f'_+(\frac{1 - x}{1 - y})$ and $(x \mapsto d_f(x, y))'_+(x) \le 0$. The opposite holds for $x \ge y$.

\end{proof}


\begin{lemma}
  \label{lem:fDiv_bounded_ge_fDiv_mean}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu \in \mathcal P([0,1])$. Then
  \begin{align*}
  D_f(\mu, \nu) \ge d_f(\mu[X], \nu[X]) \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{thm:fDiv_data_proc_2}
Let $u$ be the uniform distribution on $[0,1]$. Then $D_f(\mu, \nu) = D_f(\mu \times u, \nu \times u)$ (by lemma TODO about kernel with identity as first coordinate).
Let $g : [0,1] \times [0,1] \to \{0,1\}$ be the measurable function with $g(x, y) = \mathbb{I}\{y \le x\}$. We also write $g$ for the corresponding deterministic kernel. By the data-processing inequality (Theorem~\ref{thm:fDiv_data_proc_2}),
\begin{align*}
D_f(\mu \times u, \nu \times u)
\ge D_f(g \circ (\mu \times u), g \circ (\nu \times u))
\: .
\end{align*}
Those two last measures are Bernoulli distributions with means $\mu[X]$ and $\nu[X]$.
\end{proof}


\begin{corollary}
  \label{cor:fDiv_ge_fDiv_mean_comp}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\kappa : \mathcal X \rightsquigarrow [0,1]$. Then
  \begin{align*}
  D_f(\mu, \nu) \ge d_f((\kappa \circ \mu)[X], (\kappa \circ \nu)[X]) \: .
  \end{align*}
\end{corollary}

\begin{proof}%\leanok
\uses{lem:fDiv_bounded_ge_fDiv_mean}
First, by the data-processing inequality (Theorem~\ref{thm:fDiv_data_proc_2}),
$D_f(\mu, \nu) \ge D_f(\kappa \circ \mu, \kappa \circ \nu)$~.
Then use Lemma~\ref{lem:fDiv_bounded_ge_fDiv_mean}.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_estimation_ge}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:bayesRisk}
  Let $\pi, \xi \in \mathcal P(\Theta)$ and $P, Q : \Theta \rightsquigarrow \mathcal X$. Suppose that the loss $\ell'$ takes values in $[0,1]$. Then
  \begin{align*}
  D_f(\pi \otimes Q, \xi \otimes P)
  &\ge d_f(\mathcal R_\pi^Q, \mathcal R_\xi^P)
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{cor:fDiv_ge_fDiv_mean_comp, lem:fDiv_bernoulli_convex}
Suppose first that $\mathcal R_\pi^Q \ge \mathcal R_\xi^P$.
Let $\hat{y}_B$ be a Bayes estimator for the estimation task $(P, y, \ell')$ with prior $\xi$. If it does not exist, the proof can be adapted by taking an estimator with risk $\varepsilon$-close to the Bayes risk for any $\varepsilon > 0$.
By Corollary~\ref{cor:fDiv_ge_fDiv_mean_comp},
\begin{align*}
D_f(\pi \otimes Q, \xi \otimes P)
&\ge d_f(\ell' \circ (y \parallel \hat{y}_B) \circ (\pi \otimes Q) [X], \ell' \circ (y \parallel \hat{y}_B) \circ (\xi \otimes P) [X])
\\
&= d_f(\ell' \circ (y \parallel \hat{y}_B) \circ (\pi \otimes Q) [X], \mathcal R_\xi^P)
\: .
\end{align*}
By definition of the Bayes risk as an infimum,
\begin{align*}
\ell' \circ (y \parallel \hat{y}_B) \circ (\pi \otimes Q) [X]
\ge \mathcal R_\pi^Q
\ge \mathcal R_\xi^P
\: .
\end{align*}
Since $x \mapsto d_f(x, y)$ is monotone on $[y, 1]$,
\begin{align*}
D_f(\pi \otimes Q, \xi \otimes P)
&\ge d_f(\ell' \circ (y \parallel \hat{y}_B) \circ (\pi \otimes Q) [X], \mathcal R_\xi^P)
\\
&\ge d_f(\mathcal R_\pi^Q, \mathcal R_\xi^P)
\: .
\end{align*}
This concludes the proof for $\mathcal R_\pi^Q \ge \mathcal R_\xi^P$. If that inequality is reversed, we proceed similarly but take the Bayes estimator for $Q$.
\end{proof}





\subsection{Convexity}

\begin{theorem}[Joint convexity]
  \label{thm:fDiv_convex}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  The function $(\mu, \nu) \mapsto D_f(\mu, \nu)$ is convex.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_comp_le_condFDiv_2}
Let $\mu_0, \mu_1, \nu_0, \nu_1$ be four measures. Let $\lambda \in [0,1]$. Let $\xi$ be the probability measure on $\{0,1\}$ with $\xi(\{1\}) = \lambda$.
Let $\kappa$ be the kernel $\{0,1\} \rightsquigarrow \mathcal X$ defined by $\kappa(0) = \mu_0$ and $\kappa(1) = \mu_1$.
Let $\eta$ be the kernel $\{0,1\} \rightsquigarrow \mathcal X$ defined by $\eta(0) = \nu_0$ and $\eta(1) = \nu_1$.
\begin{align*}
D_f(\xi \otimes \kappa, \xi \otimes \eta)
&= D_f(\kappa, \eta \mid \xi)
= (1 - \lambda) D_f(\mu_0, \nu_0) + \lambda D_f(\mu_1, \nu_1)
\: , \\
D_f(\kappa \circ \xi, \eta \circ \xi)
&= D_f((1 - \lambda)\mu_0 + \lambda \mu_1, (1 - \lambda)\nu_0 + \lambda \nu_1)
\: .
\end{align*}
By Theorem~\ref{thm:fDiv_comp_le_condFDiv_2}, $D_f(\kappa \circ \xi, \eta \circ \xi) \le D_f(\xi \otimes \kappa, \xi \otimes \eta)$.
\end{proof}



\subsection{Variational representations}

TODO




\subsection{Comparisons between divergences}

Specific divergences are discussed in the next sections. We collate here general tools that can be used to compare $D_f$ and $D_g$ for two functions $f$ and $g$.

\begin{lemma}
  \label{lem:fDiv_mono_fun}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  If $f \le g$, then $D_f(\mu, \nu) \le D_g(\mu, \nu)$.
\end{lemma}

\begin{proof}
Monotonicity of the integral and of $f \mapsto f'(\infty)$.
\end{proof}


\begin{lemma}
  \label{lem:fDiv_le_of_deriv2_le}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  If $f(1) = 0$, $g(1) = 0$, $f'(1) = 0$, $g'(1) = 0$, and both $f$ and $g$ have a second derivative, then
  \begin{align*}
  D_f(\mu, \nu) \le \sup_x \frac{f''(x)}{g''(x)} D_g(\mu, \nu)
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:fDiv_mono_fun}
By Taylor with integral remainder, if $f'' \le \beta g''$,
\begin{align*}
f(x) = \int_1^x f''(t) (x - t) dt
\le \beta \int_1^x g''(t) (x - t) dt
= \beta g(x)
\: .
\end{align*}
Then use Lemma~\ref{lem:fDiv_mono_fun}.
\end{proof}


\begin{theorem}[\cite{sason2016f}]
  \label{thm:fDiv_eq_sup_mul_fDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Suppose that $f(1) = g(1) = 0$ and that $f'(1) = g'(1) = 0$, and that $g>0$ on $(0,1) \cup (1, +\infty)$. Suppose that the space $\mathcal X$ has at least two disjoint non-empty measurable sets. Then
  \begin{align*}
  \sup_{\mu, \nu \in \mathcal P(\mathcal X)} \frac{D_f(\mu, \nu)}{D_g(\mu, \nu)}
  = \sup_{t > 0, t \ne 1} \frac{f(t)}{g(t)}
  \: .
  \end{align*}
\end{theorem}

Remark: if $\mathcal X$ does not have two disjoint non-empty measurable sets then its measurable sets are only $\{\emptyset, \mathcal X\}$ and the ratio on the left is $0/0$.

\begin{proof}%\leanok
\uses{lem:fDiv_mono_fun}
TODO: proof done for measurable singletons?

By Lemma~\ref{lem:fDiv_mono_fun}, $\sup_{\mu, \nu \in \mathcal P(\mathcal X)} \frac{D_f(\mu, \nu)}{D_g(\mu, \nu)} \le \sup_{t > 0, t \ne 1} \frac{f(t)}{g(t)}$. Let's prove the other inequality.

Let $x$ and $y$ be two points in disjoint non-empty measurable sets. For $t \in (0,1) \cup (1, +\infty)$ and $\varepsilon \in (0, 1/t]$ let $\mu_{t, \varepsilon} = t \varepsilon \delta_x + (1 - t \varepsilon) \delta_y$ and $\nu_\varepsilon = \varepsilon \delta_x + (1 - \varepsilon) \delta_y$~.
\begin{align*}
D_f(\mu_{t, \varepsilon}, \nu_\varepsilon)
&= \varepsilon f(t) + (1 - \varepsilon) f(\frac{1 - t \varepsilon}{1 - \varepsilon})
\\
&= \varepsilon f(t) + \varepsilon (1 - t) \frac{1 - \varepsilon}{\varepsilon (1 - t)} f(1 + \frac{\varepsilon(1 - t)}{1 - \varepsilon})
\end{align*}
Let $\alpha = \frac{\varepsilon (1 - t)}{1 - \varepsilon}$. Then since $f(1) = g(1) = 0$,
\begin{align*}
\frac{D_f(\mu_{t, \varepsilon}, \nu_\varepsilon)}{D_g(\mu_{t, \varepsilon}, \nu_\varepsilon)}
&= \frac{f(t) + (1 - t)\frac{f(1 + \alpha) - f(1)}{\alpha}}{g(t) + (1 - t)\frac{g(1 + \alpha) - g(1)}{\alpha}} \: .
\end{align*}
As $\varepsilon \to 0$, $\alpha$ also tends to 0 and
\begin{align*}
\frac{D_f(\mu_{t, \varepsilon}, \nu_\varepsilon)}{D_g(\mu_{t, \varepsilon}, \nu_\varepsilon)}
\to \frac{f(t) + (1 - t)f'(1)}{g(t) + (1 - t)g'(1)}
= \frac{f(t)}{g(t)}
\: .
\end{align*}
\end{proof}

\paragraph{Bretagnolle-Huber inequalities}

\begin{theorem}
  \label{thm:bh_fDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:TV}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$. If $f(1) = 0$,
  \begin{align*}
  D_f(\mu, \nu)
  &\ge f(1 + \TV(\mu, \nu)) + f(1 - \TV(\mu, \nu))
  \: , \\
  \text{ and }
  D_f(\mu, \nu)
  &\ge (1 + \TV(\mu, \nu))f\left(\frac{1}{1 + \TV(\mu, \nu)}\right) + (1 - \TV(\mu, \nu))f\left(\frac{1}{1 - \TV(\mu, \nu)}\right)
  \: .
  \end{align*}
\end{theorem}

\begin{proof}%\leanok
\uses{lem:tv_eq_integral}
Let $V: x \mapsto \max\left\{0, \frac{d\mu}{d\nu}(x) - 1\right\}$ and $W: x \mapsto \max\left\{0, 1 - \frac{d\mu}{d\nu}(x)\right\}$~.
Then one of $1+V$ and $1-W$ is equal to $\frac{d\mu}{d\nu}$ and the other is equal to 1.
Since $f(1) = 0$, we obtain $f(\frac{d\mu}{d\nu}) = f(1+V) + f(1-W)$~.
The $f$-divergence is then
\begin{align*}
D_f(\mu, \nu)
&= \nu[f(\frac{d\mu}{d\nu})] + f'(\infty)\mu_{\perp \nu}(\mathcal X)
\\
&= \nu\left[f(1+V) + f'(\infty)\mu_{\perp \nu}(\mathcal X)\right] + \nu\left[f(1 - W)\right]
\: .
\end{align*}
By convexity of $f$, $f(1+V) + f'(\infty)\mu_{\perp \nu}(\mathcal X) \ge f(1+V + \mu_{\perp \nu}(\mathcal X))$~. Using convexity again, we get
\begin{align*}
D_f(\mu, \nu)
&\ge f(1+ \nu[V] + \mu_{\perp \nu}(\mathcal X)) + f(1 - \nu[W])
\: .
\end{align*}
By Lemma~\ref{lem:tv_eq_integral}, $\nu[V] + \mu_{\perp \nu}(\mathcal X) = \nu[W] = \TV(\mu, \nu)$. We obtain the first equality.

To get the second equality, remark that $D_f(\mu, \nu) = D_{x\mapsto xf(1/x)}(\nu, \mu)$ (Theorem TODO), apply the first inequality to that function and use the symmetry of $\TV$.
\end{proof}

\begin{corollary}[Bretagnolle-Huber inequality]
  \label{cor:bh_kl}
  %\lean{}
  %\leanok
  \uses{def:KL, def:TV}
  TODO: move this somewhere after the definition of $KL$.

  For $\mu, \nu \in \mathcal P(\mathcal X)$,
  \begin{align*}
  \KL(\mu, \nu)
  &\ge - \log(1 - \TV^2(\mu, \nu))
  \: , \\
  \text{or equivalently, }
  \TV(\mu, \nu)
  &\le \sqrt{1 - \exp\left(-\KL(\mu, \nu) \right)}
  \: .
  \end{align*}
\end{corollary}

\begin{proof}%\leanok
\uses{thm:bh_fDiv}
Use the second inequality of Theorem~\ref{thm:bh_fDiv}, for $f: x \mapsto x \log x$~.
\end{proof}

\begin{corollary}
  \label{cor:cor:bh_hellingerAlpha}
  %\lean{}
  %\leanok
  \uses{def:hellingerAlpha, def:TV}
  TODO: move this somewhere after the definition of $\He_\alpha$.

  Let $\mu, \nu \in \mathcal P(\mathcal X)$. For $\alpha > 0$,
  \begin{align*}
  \He_{1+\alpha}(\mu, \nu)
  &\ge \frac{1}{\alpha} \left( (1 - \TV(\mu, \nu))^{-\alpha} - 2 \right)
  \: , \\
  \text{or equivalently, }
  1 - \TV(\mu, \nu)
  &\ge \exp\left(-\frac{1}{\alpha} \log \left( 2 + \alpha \He_{1+\alpha}(\mu, \nu) \right)\right)
  \: .
  \end{align*}
\end{corollary}

\begin{proof}%\leanok
\uses{thm:bh_fDiv}
Use the second inequality of Theorem~\ref{thm:bh_fDiv}, for $f: x \mapsto \frac{x^{1+\alpha} - 1}{\alpha}$ to get
\begin{align*}
\He_{1+\alpha}(\mu, \nu) \ge \frac{1}{\alpha} \left((1 + \TV(\mu, \nu))^{-\alpha} + (1 - \TV(\mu, \nu))^{-\alpha} - 2 \right) \: .
\end{align*}
We then neglect one term: $(1 + \TV(\mu, \nu))^{-\alpha} \ge 0$.
\end{proof}


\paragraph{Joint range}

TODO \cite{harremoes2011pairs}


\subsection{Statistical divergences}

\begin{lemma}
  \label{lem:statInfo_eq_fDiv}
  %\lean{}
  %\leanok
  \uses{def:statInfo, def:fDiv}
  On probability measures, the statistical information $\mathcal I_\xi$ is an $f$-divergence for the function $\phi_{\xi_0, \xi_1}$ of Definition~\ref{def:statInfoFun}.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:statInfo_eq_integral}
This is a reformulation of Theorem~\ref{thm:statInfo_eq_integral}.
\end{proof}

% \begin{lemma}
%   \label{lem:deGrootInfo_eq_fDiv}
%   %\lean{}
%   %\leanok
%   \uses{def:deGrootInfo, def:statInfoFun, def:fDiv}
%   On probability measures, the DeGroot statistical information $I_\pi$ is an $f$-divergence for the function $\phi_{\pi, 1-\pi}$ of Definition~\ref{def:statInfoFun}.
% \end{lemma}

% \begin{proof}%\leanok
% \uses{lem:deGrootInfo_eq_integral}
% This is a reformulation of Lemma~\ref{lem:deGrootInfo_eq_integral}.
% \end{proof}

% \begin{lemma}
%   \label{lem:eGamma_eq_fDiv}
%   %\lean{}
%   %\leanok
%   \uses{def:eGamma, def:statInfoFun, def:fDiv}
%   On probability measures, the hockey-stick divergence $E_\gamma$ is an $f$-divergence for the function $\phi_{1,\gamma}$ of Definition~\ref{def:statInfoFun}.
% \end{lemma}

% \begin{proof}%\leanok
% \uses{lem:eGamma_eq_integral}
% This is a reformulation of Lemma~\ref{lem:eGamma_eq_integral}.
% \end{proof}

\begin{lemma}
  \label{lem:tv_eq_fDiv}
  %\lean{}
  %\leanok
  \uses{def:TV, def:fDiv}
  On probability measures, the total variation distance $\TV$ is an $f$-divergence for the function $x \mapsto \frac{1}{2}\vert x - 1 \vert$.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:tv_eq_integral_abs}
This is a reformulation of Lemma~\ref{lem:tv_eq_integral_abs}.
\end{proof}