\section{Testing error lower bounds}

In simple hypothesis testing, a sample from an unknown measure on $\mathcal X$ among $\{\mu, \nu\}$ is observed, and the goal of the test is to return the measure from which the sample came.
In typical scenarios, $\mathcal X$ is a product space $\mathcal Y^n$, $\mu$ and $\nu$ are product measures and the observation is a collection of $n$ i.i.d. samples.

A test is a measurable function $\phi : \mathcal X \to \{0,1\}$, which can equivalently be described by the even $E = \{\phi = 1\}$. If we decide that the test should return 0 if the sample came from $\mu$ and 1 if it came from $\nu$, the two error probabilities of the test are $\mu(E)$ and $\nu(E^c)$. A good test minimizes these (or a given combination of these).

In this section, we prove lower bounds on functions of $\mu(E)$ and $\nu(E^c)$, for any event $E$.
For the sum $\mu(E) + \nu(E^c)$, the value of the optimal test is exactly $1 - \TV(\mu, \nu)$, as shown in the following theorem.

\begin{theorem}
  \label{thm:testing_eq_tv}
  %\lean{}
  %\leanok
  \uses{def:TV}
  Let $\mu, \nu$ be two probability measures. Then $\inf_{E \text{ event}}\{\mu(E) + \nu(E^c)\} = 1 - \TV(\mu, \nu)$~.
\end{theorem}

\begin{proof}
\uses{lem:tv_eq_sub_inf_event}
This is a special case of Lemma~\ref{lem:tv_eq_sub_inf_event}.
\end{proof}

The drawback of $\TV$ is that it is not easy to manipulate. In particular, it does not tensorize. In order to quantify error probabilities on product spaces like $\mu^{\otimes n}(E) + \nu^{\otimes n}(E^c)$, we use other divergences.

\subsection{Generic lower bounds}

\begin{lemma}
  \label{lem:testing_bound_renyi_mean}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Let $\alpha \in (0,1)$. Then
  \begin{align*}
  \mu(E)^\alpha + \nu(E^c)^{1 - \alpha}
  \ge \exp\left(-(1 - \alpha) R_{\alpha}(\mu, \nu)\right)
  \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:renyi_data_proc_event}
Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$.
By Lemma~\ref{lem:renyi_data_proc_event}, $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$. That divergence is
\begin{align*}
R_\alpha(\mu_E, \nu_E)
&= \frac{1}{\alpha - 1}\log \left(\mu(E)^\alpha \nu(E)^{1 - \alpha}
  + \mu(E^c)^\alpha \nu(E^c)^{1 - \alpha}\right)
\\
&\ge \frac{1}{\alpha - 1}\log \left(\mu(E)^\alpha + \nu(E^c)^{1 - \alpha}\right)
\: .
\end{align*}
\end{proof}

\begin{corollary}
  \label{cor:testing_bound_hellinger}
  %\lean{}
  %\leanok
  \uses{def:Hellinger, def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Then
  \begin{align*}
  \sqrt{\mu(E)} + \sqrt{\nu(E^c)}
  \ge \exp\left(-\frac{1}{2} R_{1/2}(\mu, \nu)\right)
  = 1 - \Hsq(\mu, \nu)
  \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:testing_bound_renyi_mean, lem:renyi_half_eq_log_hellinger}
The inequality is an application of Lemma~\ref{lem:testing_bound_renyi_mean} for $\alpha = 1/2$. The equality is Lemma~\ref{lem:renyi_half_eq_log_hellinger}.
\end{proof}

\subsection{Change of measure}

\begin{lemma}
  \label{lem:llr_change_measure}
  \lean{ProbabilityTheory.measure_sub_le_measure_mul_exp'}
  \leanok
  %\uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$ with $\mu \ll \nu$ and let $E$ be an event on $\mathcal X$. Let $\beta \in \mathbb{R}$. Then
  \begin{align*}
  \nu(E) e^{\beta} \ge \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} > \beta \right\} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\begin{align*}
\nu(E)
\ge \mu\left[\mathbb{I}(E) e^{- \log\frac{d \mu}{d \nu} }\right]
&\ge \mu\left[\mathbb{I}\left(E \cap \left\{\log\frac{d \mu}{d \nu} \le \beta\right\}\right) e^{- \log\frac{d \mu}{d \nu} }\right]
\\
&\ge e^{- \beta}\mu\left(E \cap \left\{\log\frac{d \mu}{d \nu} \le \beta\right\}\right)
\\
&\ge e^{- \beta}\left( \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} > \beta \right\} \right)
\: .
\end{align*}
\end{proof}

\begin{corollary}
  \label{cor:kl_change_measure}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\beta \in \mathbb{R}$. Then
  \begin{align*}
  \nu(E) e^{\KL(\mu, \nu) + \beta} \ge \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} - \KL(\mu, \nu) > \beta \right\} \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:llr_change_measure}
Use Lemma~\ref{lem:llr_change_measure} with the choice $\KL(\mu, \nu) + \beta$ for $\beta$.
\end{proof}

\begin{lemma}
  \label{lem:llr_change_measure_variance}
  %\lean{}
  %\leanok
  \uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$ such that $\mu\left[\left(\log\frac{d \mu}{d \nu}\right)^2\right] < \infty$. Let $E$ be an event on $\mathcal X$ and let $\beta > 0$. Then
  \begin{align*}
  \nu(E) e^{\KL(\mu, \nu) + \sqrt{\Var_\mu[\log\frac{d \mu}{d \nu}]\beta}} \ge \mu(E) - \frac{1}{\beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:llr_change_measure}
Use Lemma~\ref{lem:llr_change_measure} with the choice $\KL(\mu, \nu) + \sqrt{\Var_\mu[\log\frac{d \mu}{d \nu}]\beta}$ for $\beta$ and bound the probability of deviation of the log-likelihood ratio with Chebychev's inequality.
\end{proof}

\begin{lemma}
  \label{lem:renyi_chernoff_bound}
  \lean{ProbabilityTheory.measure_llr_gt_renyiDiv_le_exp}
  \leanok
  \uses{def:Renyi}
  For $\mu, \nu$ finite measures and $\alpha, \beta > 0$,
  \begin{align*}
  \mu\left\{ \log\frac{d \mu}{d \nu} > R_{1+\alpha}(\mu, \nu) + \beta \right\}
  \le e^{- \alpha \beta}
  \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:renyi_cgf_2}
This is a Chernoff bound, using that the cumulant generating function of $\log\frac{d\mu}{d\nu}$ under $\mu$ has value $\alpha R_{1+\alpha}(\mu, \nu)$ at $\alpha$ by Lemma~\ref{lem:renyi_cgf_2}.
\begin{align*}
\mu\left\{ \log\frac{d \mu}{d \nu} > R_{1+\alpha}(\mu, \nu) + \beta \right\}
&= \mu\left\{ \exp\left(\alpha\log\frac{d \mu}{d \nu}\right) > \exp\left(\alpha R_{1+\alpha}(\mu, \nu) + \alpha \beta\right) \right\}
\\
&\le e^{-\alpha R_{1+\alpha}(\mu, \nu) - \alpha \beta} \mu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha \right]
\end{align*}
Then $\mu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha \right] = \nu\left[\left(\frac{d \mu}{d \nu}\right)^{1+\alpha} \right] = e^{\alpha R_{1+\alpha}(\mu, \nu)}$.
\end{proof}

\begin{lemma}
  \label{lem:renyi_change_measure}
  \lean{ProbabilityTheory.measure_sub_le_measure_mul_exp_renyiDiv}
  \leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha,\beta > 0$. Then
  \begin{align*}
  \nu(E) e^{R_{1+\alpha}(\mu, \nu) + \beta} \ge \mu(E) - e^{-\alpha \beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure, lem:renyi_chernoff_bound}
Use Lemma~\ref{lem:llr_change_measure} with the choice $R_{1+\alpha}(\mu, \nu) + \beta$ for $\beta$. Then apply Lemma~\ref{lem:renyi_chernoff_bound}.
\end{proof}

\begin{lemma}
  \label{lem:llr_change_measure_add}
  \lean{ProbabilityTheory.one_sub_le_add_measure_mul_exp}
  \leanok
  %\uses{}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\beta_1, \beta_2 \in \mathbb{R}$. Then
  \begin{align*}
  \mu(E) e^{\beta_1} + \nu(E^c) e^{\beta_2} \ge 1 - \xi\left\{ \log\frac{d \xi}{d \mu} > \beta_1 \right\} - \xi\left\{ \log\frac{d \xi}{d \nu} > \beta_2 \right\} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure}
Two applications of Lemma~\ref{lem:llr_change_measure}, then sum them and use $\xi(E)+\xi(E^c) = 1$.
\end{proof}

\begin{lemma}
  \label{lem:change_measure_variance_add}
  % \lean{}
  % \leanok
  \uses{}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. For $\beta > 0$~,
  \begin{align*}
  \mu(E) e^{\KL(\xi, \mu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\mu}\right]}} + \nu(E^c) e^{\KL(\xi, \nu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\nu}\right]}}
  \ge 1 - \frac{2}{\beta} \: .
  \end{align*}
\end{lemma}

\begin{proof} %\leanok
\uses{lem:llr_change_measure_add}
Use Lemma~\ref{lem:llr_change_measure_add} with the choices $\KL(\xi, \mu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\mu}\right]}$ and $\KL(\xi, \nu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\nu}\right]}$ for $\beta_1$ and $\beta_2$.
Then use Chebyshev's inequality to bound the probabilities of deviation of the log-likelihood ratios.
\end{proof}

\begin{lemma}
  \label{lem:renyi_change_measure_add}
  \lean{ProbabilityTheory.one_sub_exp_le_add_measure_mul_exp_max_renyiDiv}
  \leanok
  \uses{def:Renyi}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha, \beta \ge 0$. Then
  \begin{align*}
  \mu(E) e^{R_{1+\alpha}(\xi, \mu) + \beta} + \nu(E^c) e^{R_{1+\alpha}(\xi, \nu) + \beta} \ge 1 - 2 e^{-\alpha \beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure_add, lem:renyi_chernoff_bound}
Use Lemma~\ref{lem:llr_change_measure_add} with the choices $R_{1+\alpha}(\xi, \mu) + \beta$ and $R_{1+\alpha}(\xi, \nu) + \beta$ for $\beta_1$ and $\beta_2$.
Then apply Lemma~\ref{lem:renyi_chernoff_bound}.
\end{proof}


\subsection{Lower bounds on the maximum}

\begin{lemma}
  \label{lem:testing_bound_tv_hellinger_max}
  %\lean{}
  %\leanok
  \uses{def:TV, def:Hellinger}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Then
  \begin{align*}
  \max\{\mu(E), \nu(E^c)\}
  \ge \frac{1}{2}(1 - \TV(\mu, \nu))
  \ge \frac{1}{4}(1 - \Hsq(\mu, \nu))^2
  \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{thm:testing_eq_tv, cor:one_sub_hellinger_squared_le_one_sub_tv}
The first inequality is Theorem~\ref{thm:testing_eq_tv} with $\mu(E) + \nu(E^c) \le 2 \max\{\mu(E), \nu(E^c)\}$.
The second inequality is a consequence of Corollary~\ref{cor:one_sub_hellinger_squared_le_one_sub_tv}.
\end{proof}

\begin{lemma}
  \label{lem:testing_bound_renyi_max}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Let $\alpha \in (0,1)$. Then
  \begin{align*}
  \min\{\alpha, 1 - \alpha\} \log\frac{1}{\max\{\mu(E), \nu(E^c)\}} \le (1 - \alpha) R_{\alpha}(\mu, \nu)  + \log 2 \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:testing_bound_renyi_mean}
Use Lemma~\ref{lem:testing_bound_renyi_mean} and remark that $\mu(E)^\alpha + \nu(E^c)^{1 - \alpha} \le 2\max\{\mu(E), \nu(E^c)\}^{\min\{\alpha, 1 - \alpha\}}$.
\end{proof}


\begin{lemma}
  \label{lem:testing_bound_renyi_one_add}
  \lean{ProbabilityTheory.exp_neg_chernoffDiv_le_add_measure}
  \leanok
  \uses{def:Renyi, def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha > 0$. Then
  \begin{align*}
  \mu(E) + \nu(E^c) \ge \frac{1}{2}\exp\left( - C_{1+\alpha}(\mu, \nu) - \frac{\log 4}{\alpha}\right) \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:renyi_change_measure_add}
\end{proof}


\subsection{Product spaces}

\begin{corollary}
  \label{cor:testing_bound_renyi_n}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event of $\mathcal X^n$. Let $\alpha \in (0,1)$. Then
  \begin{align*}
  \min\{\alpha, 1 - \alpha\} \log\frac{1}{\max\{\mu^{\otimes n}(E), \nu^{\otimes n}(E^c)\}} \le (1 - \alpha) n R_{\alpha}(\mu, \nu)  + \log 2 \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:testing_bound_renyi_max, lem:renyi_prod_n}
Use Lemma~\ref{lem:renyi_prod_n} in Lemma~\ref{lem:testing_bound_renyi_max}.
\end{proof}

\begin{lemma}
  \label{lem:testing_bound_renyi_one_add_n}
  %\lean{}
  %\leanok
  \uses{def:Renyi, def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$, let $n \in \mathbb{N}$ and let $E$ be an event on $\mathcal X^n$. For all $\alpha > 0$,
  \begin{align*}
  \mu^{\otimes n}(E) + \nu^{\otimes n}(E^c) \ge \frac{1}{2}\exp\left( - n C_{1+\frac{\alpha}{\sqrt{n}}}(\mu, \nu) - \frac{\log 4}{\alpha}\sqrt{n}\right) \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:renyi_prod_n, lem:testing_bound_renyi_one_add}
Use Lemma~\ref{lem:renyi_prod_n} in Lemma~\ref{lem:testing_bound_renyi_one_add}. TODO: add a lemma for tensorization of the Chernoff divergence.
\end{proof}

\begin{theorem}
  \label{thm:testing_bound_chernoff}
  %\lean{}
  %\leanok
  \uses{def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and let $(E_n)_{n \in \mathbb{N}}$ be events on $\mathcal X^n$. For all $\gamma \in (0,1)$,
  \begin{align*}
  \limsup_{n \to +\infty} \frac{1}{n}\log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)}
  \le C_1(\mu, \nu)
  \: .
  \end{align*}
\end{theorem}

\begin{proof}
\uses{cor:kl_change_measure, thm:kl_pi}
Let $\xi$ be a probability measure on $\mathcal X$ and $\beta > 0$. By Corollary~\ref{cor:kl_change_measure},
\begin{align*}
\mu^{\otimes n}(E_n) e^{\KL(\xi^{\otimes n}, \mu^{\otimes n}) + n\beta}
&\ge \xi^{\otimes n}(E_n) - \xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \mu^{\otimes n}} - \KL(\xi^{\otimes n}, \mu^{\otimes n}) > n\beta \right\}
\: , \\
\nu^{\otimes n}(E_n^c) e^{\KL(\xi^{\otimes n}, \nu^{\otimes n}) + n\beta}
&\ge \xi^{\otimes n}(E_n^c) - \xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \nu^{\otimes n}} - \KL(\xi^{\otimes n}, \nu^{\otimes n}) > n\beta \right\}
\: .
\end{align*}
We sum both inequalities with weights $\gamma$ and $1-\gamma$ respectively and use that each $\KL$ on the left is less than their max, as well as $\xi^{\otimes n}(E_n) + \xi^{\otimes n}(E_n^c) = 1$.
\begin{align*}
&e^{n\beta} (\gamma\mu^{\otimes n}(E_n) + (1-\gamma)\nu^{\otimes n}(E_n^c)) e^{\max\{\KL(\xi^{\otimes n}, \mu^{\otimes n}), \KL(\xi^{\otimes n}, \nu^{\otimes n})\}}
\\
&\ge \min\{\gamma, 1-\gamma\} - \gamma\xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \mu^{\otimes n}} - \KL(\xi^{\otimes n}, \mu^{\otimes n}) > n\beta \right\}
  - (1 - \gamma)\xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \nu^{\otimes n}} - \KL(\xi^{\otimes n}, \nu^{\otimes n}) > n\beta \right\}
\: .
\end{align*}
Let $p_{n,\mu}(\beta)$ and $p_{n, \nu}(\beta)$ be the two probabilities on the right hand side. By the law of large numbers, both tend to 0 when $n$ tends to $+\infty$.
In particular, for $n$ large enough, the right hand side is positive and we can take logarithms on both sides. We also use the tensorization of $\KL$ (Theorem~\ref{thm:kl_pi}).
\begin{align*}
& n \max\{\KL(\xi, \mu), \KL(\xi, \nu)\}
\\
&\ge \log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)} + \log (\min\{\gamma, 1-\gamma\} - \gamma p_{n, \mu}(\beta) - (1 - \gamma) p_{n, \nu}(\beta)) - n\beta
\end{align*}
For $n \to +\infty$,
\begin{align*}
\max\{\KL(\xi, \mu), \KL(\xi, \nu)\}
\ge \limsup_{n \to + \infty}\frac{1}{n}\log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)} - \beta
\end{align*}
Since $\beta > 0$ is arbitrary, we can take a supremum over $\beta$ on the right.
\end{proof}



\section{Sample complexity}

An estimation problem of the shape $(P^{\otimes n}, y, \ell')$ corresponds to estimating from $n$ i.i.d. samples of the distribution. From the data-processing inequality, we get that the risk (Bayes risk, minimax risk...) decreases with the number of samples (Lemma~\ref{lem:bayesRisk_mono_prod} for example).
Intuitively, having more information allows for better estimation.

In a sequence of estimation tasks $(P^{\otimes n}, y, \ell')$ for $n \in \mathbb{N}$, the \emph{sample complexity} is the minimal $n$ such that the risk is less than a desired value.

\begin{lemma}
  \label{lem:bayesRisk_mono_prod}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  If $n \le m$ then $\mathcal R_\pi^{P^{\otimes n}} \ge \mathcal R_\pi^{P^{\otimes m}}$.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:data_proc_bayesRisk}
Use the data-processing inequality, Theorem~\ref{thm:data_proc_bayesRisk}.
\end{proof}


\begin{definition}
  \label{def:priorSampleComplexity}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  The sample complexity of Bayesian estimation with respect to a prior $\pi \in \mathcal M(\Theta)$ at risk level $\delta \in \mathbb{R}_{+,\infty}$ is
  \begin{align*}
  n_\pi^P(\delta) = \min \{n \in \mathbb{N} \mid \mathcal R_\pi^{P^{\otimes n}} \le \delta\} \: .
  \end{align*}
\end{definition}


\begin{definition}
  \label{def:binaryPriorSampleComplexity}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:priorSampleComplexity}
  The sample complexity of simple binary hypothesis testing with prior $(\pi, 1 - \pi) \in \mathcal P(\{0, 1\})$ at risk level $\delta \in \mathbb{R}_{+, \infty}$ is
  \begin{align*}
  n(\mu, \nu, \pi, \delta) = \min\{n \in \mathbb{N} \mid B_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \le \delta\} \: .
  \end{align*}
  This is the sample complexity $n_\xi^P(\delta)$ specialized to simple binary hypothesis testing.
\end{definition}


\begin{lemma}
  \label{lem:binaryPriorSampleComplexity_eq_zero}
  %\lean{}
  %\leanok
  \uses{def:binaryPriorSampleComplexity}
  For $\delta \ge \min\{\pi, 1 - \pi\}$, the sample complexity of simple binary hypothesis testing is
  $n(\mu, \nu, \pi, \delta) = 0$~.
\end{lemma}

\begin{proof}
\uses{lem:bayesBinaryRisk_le}
\end{proof}


\begin{lemma}
  \label{lem:binaryPriorSampleComplexity_le_renyi}
  %\lean{}
  %\leanok
  \uses{def:Renyi, def:binaryPriorSampleComplexity}
  The sample complexity of simple binary hypothesis testing satisfies $n(\mu, \nu, \pi, \delta) \le n_0$~, with $n_0$ the smallest natural number such that
  \begin{align*}
  \log\frac{1}{\delta}
  \le \sup_{\alpha \in (0,1)} \left( n_0 (1 - \alpha)R_\alpha(\mu, \nu) + \alpha\log\frac{1}{\pi} + (1 - \alpha)\log\frac{1}{1 - \pi} \right)
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{cor:bayesBinaryRisk_le_exp_renyi, lem:renyi_prod_n}
It suffices to show that $B_\pi(\mu^{\otimes n_0}, \nu^{\otimes n_0}) \le \delta$~.
By Corollary~\ref{cor:bayesBinaryRisk_le_exp_renyi}, for all $\alpha \in (0,1)$,
\begin{align*}
B_\pi(\mu^{\otimes n_0}, \nu^{\otimes n_0})
&\le \pi^{\alpha} (1 - \pi)^{1 - \alpha}e^{- (1 - \alpha)R_\alpha(\mu^{\otimes n_0}, \nu^{\otimes n_0})}
\: .
\end{align*}
The RÃ©nyi divergence tensorizes: $R_\alpha(\mu^{\otimes n_0}, \nu^{\otimes n_0}) = n_0 R_\alpha(\mu, \nu)$~.
\begin{align*}
B_\pi(\mu^{\otimes n_0}, \nu^{\otimes n_0})
&\le \exp\left(- \left( n_0 (1 - \alpha)R_\alpha(\mu, \nu) + \alpha\log\frac{1}{\pi} + (1 - \alpha)\log\frac{1}{1 - \pi} \right) \right)
\: .
\end{align*}
By definition, $n_0$ is such that the infimum over $\alpha$ of the right hand side is less than $\delta$.
\end{proof}


\begin{lemma}
  \label{lem:bayesBinaryRisk_bernoulli}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk}
  Let $\hat{y}_B$ be the generalized Bayes estimator for simple binary hypothesis testing.
  The distribution $\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi \otimes P)$ (in which $\ell$ stands for the associated deterministic kernel) is a Bernoulli with mean $\mathcal B_\pi(\mu, \nu)$.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}


\begin{lemma}[\cite{pensia2024sample}]
  \label{lem:sub_bayesBinaryRisk_le_jensenShannon}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:jensenShannon}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\alpha \in (0, 1)$.
  \begin{align*}
  h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) \le \JS_\alpha(\mu, \nu) \: ,
  \end{align*}
  in which $h_2: x \mapsto x\log\frac{1}{x} + (1 - x)\log\frac{1}{1 - x}$ is the binary entropy function.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:kl_data_proc, thm:kl_compProd, lem:jensenShannon_prod_n, lem:jensenShannon_eq_kl, lem:bayesBinaryRisk_bernoulli}

TODO: split lemmas out of this proof.

Let $P : \{0,1\} \rightsquigarrow \mathcal X$ be the kernel with $P(0) = \mu$ and $P(1) = \nu$.
Let $u \in \mathcal P(\{0,1\})$ be the probability distribution with $u_0 = 1/2$. We write $\pi_\alpha$ for the probability measure on $\{0,1\}$ with $\pi_0 = \alpha$. By the chain rule (Theorem~\ref{thm:kl_compProd}) and Lemma~\ref{lem:jensenShannon_eq_kl},
\begin{align*}
\KL(\pi_\alpha \otimes P, u \times (P \circ \pi_\alpha))
&= \kl(\alpha, 1/2) + \KL(\pi_\alpha \otimes P, \pi_\alpha \times (P \circ \pi_\alpha))
\\
&= \kl(\alpha, 1/2) + \JS_\alpha(\mu, \nu)
\\
&= \log 2 - h_2(\alpha) + \JS_\alpha(\mu, \nu)
\: .
\end{align*}

Let $\hat{y}_B$ be the generalized Bayes estimator. By the data-processing inequality for $\KL$ (Theorem~\ref{thm:kl_data_proc}),
\begin{align*}
\KL(\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \otimes P), \ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (u \times (P \circ \pi_\alpha)))
\le \KL(\pi_\alpha \otimes P, u \times (P \circ \pi_\alpha))
\: .
\end{align*}
The two measures on the left are probability measures on $\{0,1\}$, with $\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (u \times (P \circ \pi_\alpha)) = u$ and $(\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \otimes P)) (\{1\}) = B_\alpha(\mu, \nu)$ (Lemma~\ref{lem:bayesBinaryRisk_bernoulli}).
We get
\begin{align*}
\KL(\pi_\alpha \otimes P, u \times (P \circ \pi_\alpha))
\ge \kl(B_\alpha(\mu, \nu), 1/2)
= \log 2 - h_2(B_\alpha(\mu, \nu))
\: .
\end{align*}
Putting things together, we proved
\begin{align*}
\JS_\alpha(\mu, \nu) \ge h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) \: .
\end{align*}
\end{proof}


\begin{lemma}
  \label{lem:kl_bayesBinaryRisk_le_jensenShannon}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:jensenShannon}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\alpha \in (0, 1)$. Then
  \begin{align*}
  \kl(B_\alpha(\mu, \nu), \min\{\alpha, 1 - \alpha\}) \le \JS_\alpha(\mu, \nu) \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:jensenShannon_eq_kl, thm:kl_data_proc, lem:bayesBinaryRisk_bernoulli}
Let $\hat{y}_B$ be the generalized Bayes estimator. By Lemma~\ref{lem:jensenShannon_eq_kl}, then the data-processing inequality for $\KL$ (Theorem~\ref{thm:kl_data_proc}),
\begin{align*}
\JS_\alpha(\mu, \nu)
&= \KL(\pi_\alpha \otimes P, \pi_\alpha \times (P \circ \pi_\alpha))
\\
&\ge \KL(\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \otimes P), \ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \times (P \circ \pi_\alpha)))
\end{align*}
$\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \otimes P)$ is Bernoulli with mean $B_\alpha(\mu, \nu)$ (Lemma~\ref{lem:bayesBinaryRisk_bernoulli}).
$\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \times (P \circ \pi_\alpha))$ is Bernoulli, with mean $\alpha(\alpha \mu(E) + (1 - \alpha) \nu(E)) + (1 - \alpha) (\alpha \mu(E^c) + (1 - \alpha) \nu(E^c))$ for some event $E$.
\begin{align*}
&\alpha(\alpha \mu(E) + (1 - \alpha) \nu(E)) + (1 - \alpha) (\alpha \mu(E^c) + (1 - \alpha) \nu(E^c))
\\
&= \alpha(\alpha \mu(E) + (1 - \alpha) \mu(E^c)) + (1 - \alpha) (\alpha \nu(E) + (1 - \alpha) \nu(E^c))
\\
&\ge \alpha \min\{\alpha, 1 - \alpha\} + (1 - \alpha) \min\{\alpha, 1 - \alpha\}
\\
&= \min\{\alpha, 1 - \alpha\}
\: .
\end{align*}
Since $B_\alpha(\mu, \nu) \le \min\{\alpha, 1 - \alpha\}$, we obtain
\begin{align*}
\kl(B_\alpha(\mu, \nu), \min\{\alpha, 1 - \alpha\})
&\le \KL(\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \otimes P), \ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \times (P \circ \pi_\alpha)))
\\
&\le \JS_\alpha(\mu, \nu)
\: .
\end{align*}
\end{proof}

Lemma~\ref{lem:kl_bayesBinaryRisk_le_jensenShannon} gives a worse conclusion than Lemma~\ref{lem:sub_bayesBinaryRisk_le_jensenShannon}. Indeed, if for example $\alpha \le 1 - \alpha$~,
\begin{align*}
\kl(B_\alpha(\mu, \nu), \min\{\alpha, 1 - \alpha\})
&= h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) + (B_\alpha(\mu, \nu) - \alpha) \log \frac{1 - \alpha}{\alpha}
\\
&\le h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) \: .
\end{align*}


\begin{lemma}
  \label{lem:binaryPriorSampleComplexity_ge_jensenShannon}
  %\lean{}
  %\leanok
  \uses{def:binaryPriorSampleComplexity, def:jensenShannon}
  For $\delta \le \min\{\pi, 1 - \pi\}$, the sample complexity of simple binary hypothesis testing satisfies
  \begin{align*}
  n(\mu, \nu, \pi, \delta) \ge \frac{h_2(\pi) - h_2(\delta)}{\JS_\pi(\mu, \nu)}
  \: ,
  \end{align*}
  in which $h_2: x \mapsto x\log\frac{1}{x} + (1 - x)\log\frac{1}{1 - x}$ is the binary entropy function.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:sub_bayesBinaryRisk_le_jensenShannon}
We start from Lemma~\ref{lem:sub_bayesBinaryRisk_le_jensenShannon}.
\begin{align*}
\JS_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \ge h_2(\pi) - h_2(B_\pi(\mu^{\otimes n}, \nu^{\otimes n})) \: .
\end{align*}
By Lemma~\ref{lem:jensenShannon_prod_n}, $\JS_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \le n \JS_\pi(\mu, \nu)$. The final result is obtained by using that $B_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \le \delta$ for $n = n(\mu, \nu, \pi, \delta)$.
\end{proof}