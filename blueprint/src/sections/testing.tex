
The goal of this section is to obtain bounds on the risk of simple binary hypothesis testing $B_\pi(\mu^{\otimes n}, \nu^{\otimes n})$ or on $\inf_{E} \max\{\mu^{\otimes n}(E), \nu^{\otimes n}(E^c)\}$ that show how those quantities depends on the number of samples $n \in \mathbb{N}$.

The main idea is to relate these to divergences that tensorize, or on which we have tensorization inequalities. Those divergences are $\KL$, $R_\alpha$, $\JS_\alpha$ (and more generally the mutual information if we want to consider more than two hypotheses).



\section{Bounding the Bayes risk with tensorizing divergences}

\subsection{Upper bound with the Chernoff information}

\begin{lemma}[\cite{zhou2018non}]
  \label{lem:bayesBinaryRisk_eq_exp_renyi_mul_integral}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:Renyi, def:renyiMeasure}
  Let $\zeta$ be a measure such that $\mu \ll \zeta$ and $\nu \ll \zeta$. Let $p = \frac{d \mu}{d\zeta}$ and $q = \frac{d \nu}{d\zeta}$.
  For $\alpha \in (0,1)$, for $g_\alpha(x) = \min\{(\alpha-1)x, \alpha x\}$,
  \begin{align*}
  \mathcal B_\xi(\mu, \nu)
  = e^{-(1 - \alpha) R_\alpha(\xi_0\mu, \xi_1\nu)} \int_x \exp \left(g_\alpha\left( \log \frac{\xi_1 q(x)}{\xi_0 p(x)} \right)\right) \partial(\xi_0\mu)^{(\alpha, \xi_1\nu)}
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{thm:bayesBinaryRisk_eq}
\begin{align*}
\mathcal B_\xi(\mu, \nu)
&= \int_x \min \left\{\xi_0 p(x), \xi_1 q(x)\right\} \partial\zeta
\\
&= \int_x (\xi_0 p(x))^\alpha (\xi_1 q(x))^{1-\alpha} \min \left\{ \left(\frac{\xi_0p(x)}{\xi_1q(x)}\right)^{1 - \alpha}, \left(\frac{\xi_1q(x)}{\xi_0p(x)}\right)^\alpha\right\} \partial\zeta
\\
&= e^{-(1 - \alpha) R_\alpha(\xi_0\mu, \xi_1\nu)} \int_x \min \left\{ \left(\frac{\xi_1q(x)}{\xi_0p(x)}\right)^{\alpha - 1}, \left(\frac{\xi_1q(x)}{\xi_0p(x)}\right)^\alpha\right\} \partial(\xi_0\mu)^{(\alpha, \xi_1\nu)}
\: .
\end{align*}
\end{proof}


\begin{corollary}
  \label{cor:bayesBinaryRisk_le_exp_renyi}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:Renyi}
  For $\alpha \in (0,1)$,
  \begin{align*}
  \mathcal B_\xi(\mu, \nu)
  \le e^{-(1 - \alpha) R_\alpha(\xi_0\mu, \xi_1\nu)}
  = \xi_0^\alpha \xi_1^{1-\alpha} e^{-(1 - \alpha) R_\alpha(\mu, \nu)}
  \: .
  \end{align*}
\end{corollary}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_eq_exp_renyi_mul_integral}
Use $g_\alpha(x) \le 0$ in Lemma~\ref{lem:bayesBinaryRisk_eq_exp_renyi_mul_integral}.
\end{proof}


\begin{lemma}
  \label{lem:bayesBinaryRisk_le_exp_chernoff}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:Chernoff}
  \begin{align*}
  \mathcal B_\xi(\mu, \nu)
  \le e^{- C_1(\xi_0\mu, \xi_1\nu)}
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{cor:bayesBinaryRisk_le_exp_renyi, lem:chernoff_eq_max_renyi}
Optimize over $\alpha$ in Corollary~\ref{cor:bayesBinaryRisk_le_exp_renyi}: the optimal value gives the Chernoff information thanks to Lemma~\ref{lem:chernoff_eq_max_renyi}.
\end{proof}


\begin{lemma}
  \label{lem:one_sub_tv_le_exp_chernoff}
  %\lean{}
  %\leanok
  \uses{def:TV, def:Chernoff}
  For probability measures,
  \begin{align*}
  1 - \TV(\mu, \nu) \le e^{- C_1(\mu, \nu)} \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_le_exp_chernoff}
By definition, since $\mu$ and $\nu$ are probability measures, $1 - \TV(\mu, \nu) = \mathcal B_{(1,1)}(\mu, \nu)$. Then apply Lemma~\ref{lem:bayesBinaryRisk_le_exp_chernoff}.
\end{proof}



\subsection{Lower bounds using the data-processing inequality}

The ideas behind the next two theorems are the same, up to the order of the arguments of a Kullback-Leibler divergence.
The hidden parameter and data in simple binary hypothesis testing are generated from a distribution $\pi \otimes P \in \mathcal P(\Theta \times \mathcal X)$.
The reason that an estimator can get low risk is that $P$ depends on the parameter: if it did not, the Bayes risk with prior $(\alpha, 1 - \alpha)$ would be $\min\{\alpha, 1 - \alpha\}$.
We will compare $\pi \otimes P$ to a product distribution $\pi' \times \xi$ for $\pi' \in \mathcal P(\Theta)$ and $\xi \in \mathcal P(\mathcal X)$.
In order to do that, we use the data-processing inequality for $\KL$, starting either from $\KL(\pi \otimes P, \pi' \times \xi)$ or from $\KL(\pi' \times \xi, \pi \otimes P)$ (the two theorems correspond to the two choices).
The data-processing inequality is used to say that those divergences are greater than the divergences between the losses of the two corresponding estimation tasks.

We start with some preparatory lemmas.


\begin{lemma}
  \label{lem:kl_bounded_ge_kl_mean}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu \in \mathcal P([0,1])$. Then
  \begin{align*}
  \KL(\mu, \nu) \ge \kl(\mu[X], \nu[X]) \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{thm:kl_data_proc}
Let $u$ be the uniform distribution on $[0,1]$. Then $\KL(\mu, \nu) = \KL(\mu \times u, \nu \times u)$ (by lemma TODO about kernel with identity as first coordinate).
Let $f : [0,1] \times [0,1] \to \{0,1\}$ be the measurable function with $f(x, y) = \mathbb{I}\{y \le x\}$. We also write $f$ for the corresponding deterministic kernel. By the data-processing inequality (Theorem~\ref{thm:kl_data_proc}),
\begin{align*}
\KL(\mu \times u, \nu \times u)
\ge \KL(f \circ (\mu \times u), f \circ (\nu \times u))
\: .
\end{align*}
Those two last measures are Bernoulli distributions with means $\mu[X]$ and $\nu[X]$.
\end{proof}


\begin{corollary}
  \label{cor:kl_ge_kl_mean_comp}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\kappa : \mathcal X \rightsquigarrow [0,1]$. Then
  \begin{align*}
  \KL(\mu, \nu) \ge \kl((\kappa \circ \mu)[X], (\kappa \circ \nu)[X]) \: .
  \end{align*}
\end{corollary}

\begin{proof}%\leanok
\uses{thm:kl_data_proc, lem:kl_bounded_ge_kl_mean}
First, by the data-processing inequality (Theorem~\ref{thm:kl_data_proc}),
$\KL(\mu, \nu) \ge \KL(\kappa \circ \mu, \kappa \circ \nu)$~.
Then uses Lemma~\ref{lem:kl_bounded_ge_kl_mean}.
\end{proof}


\begin{lemma}
  \label{lem:kl_estimation_ge}
  %\lean{}
  %\leanok
  \uses{def:KL, def:bayesBinaryRisk}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\alpha, \beta \in (0, 1)$. Let $P, Q : \{0,1\} \rightsquigarrow \mathcal X$. We write $\pi_\alpha$ for the probability measure on $\{0,1\}$ with $\pi_\alpha(\{0\}) = \alpha$. Then
  \begin{align*}
  \KL(\pi_\beta \otimes Q, \pi_\alpha \otimes P)
  &\ge \kl(B_\beta(Q(0), Q(1)), B_\alpha(P(0), P(1)))
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{thm:kl_data_proc, lem:bayesBinaryRisk_bernoulli}
Suppose first that $B_\beta(Q(0), Q(1)) \ge B_\alpha(P(0), P(1))$.
Let $\hat{y}_B$ be the generalized Bayes estimator for the simple binary hypothesis testing task $(P, y, \ell)$, which is deterministic and returns 1 for $x \in E$ for some event $E$ and 0 otherwise. By the data-processing inequality for $\KL$ (Theorem~\ref{thm:kl_data_proc}),
\begin{align*}
\KL(\pi_\beta \otimes Q, \pi_\alpha \otimes P)
&\ge \KL(\ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\beta \otimes Q), \ell \circ (\mathrm{id} \parallel \hat{y}_B) \circ (\pi_\alpha \otimes P))
\\
&= \kl(\beta Q(0)(E) + (1 - \beta) Q(1)(E^c), B_\alpha(P(0), P(1)))
\end{align*}
Since $\inf_{E} [\beta Q(0)(E) + (1 - \beta) Q(1)(E^c)] = B_\beta(Q(0), Q(1)) \ge B_\alpha(P(0), P(1))$ and $x \mapsto \kl(x, y)$ is monotone on $[y, 1]$,
\begin{align*}
\KL(\pi_\beta \otimes Q, \pi_\alpha \otimes P)
&\ge \kl(\inf_{E} [\beta Q(0)(E) + (1 - \beta) Q(1)(E^c)], B_\alpha(P(0), P(1)))
\\
&= \kl(B_\beta(Q(0), Q(1)), B_\alpha(P(0), P(1)))
\: .
\end{align*}
This concludes the proof for $B_\beta(Q(0), Q(1)) \ge B_\alpha(P(0), P(1))$. If that inequality is reversed, we proceed similarly but take the Bayes estimator for $Q$.
\end{proof}


\begin{corollary}
  \label{cor:kl_estimation_ge_sub_kl}
  %\lean{}
  %\leanok
  \uses{def:KL, def:bayesBinaryRisk}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\alpha, \beta \in (0, 1)$. Let $P, Q : \{0,1\} \rightsquigarrow \mathcal X$. We write $\pi_\alpha$ for the probability measure on $\{0,1\}$ with $\pi_\alpha(\{0\}) = \alpha$. Then
  \begin{align*}
  \KL(\pi_\beta \otimes Q, \pi_\beta \otimes P)
  &\ge \kl(B_\beta(Q(0), Q(1)), B_\alpha(P(0), P(1))) - \kl(\beta, \alpha)
  \: .
  \end{align*}
\end{corollary}

\begin{proof}%\leanok
\uses{lem:kl_estimation_ge, thm:kl_compProd}
Use $\KL(\pi_\beta \otimes Q, \pi_\alpha \otimes P) = \kl(\beta, \alpha) + \KL(\pi_\beta \otimes Q, \pi_\beta \otimes P)$ in Lemma~\ref{lem:kl_estimation_ge}.
\end{proof}


\begin{lemma}
  \label{lem:kl_estimation_ge_prod}
  %\lean{}
  %\leanok
  \uses{def:KL, def:bayesBinaryRisk}
  Let $\mu, \nu, \xi \in \mathcal P(\mathcal X)$ and let $\alpha, \beta \in (0, 1)$. Let $P : \{0,1\} \rightsquigarrow \mathcal X$ be the kernel with $P(0) = \mu$ and $P(1) = \nu$. We write $\pi_\alpha$ for the probability measure on $\{0,1\}$ with $\pi_\alpha(\{0\}) = \alpha$. Let $\bar{\beta} = \min\{\beta, 1 - \beta\}$.
  Then
  \begin{align*}
  \KL(\pi_\alpha \otimes P, \pi_\alpha \times \xi)
  &\ge \kl(B_\alpha(\mu, \nu), \bar{\beta}) - \kl(\alpha, \bar{\beta})
  \: , \\
  \KL(\pi_\beta \times \xi, \pi_\beta \otimes P)
  &\ge \kl(\bar{\beta}, B_\alpha(\mu, \nu)) - \kl(\bar{\beta}, \alpha)
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{cor:kl_estimation_ge_sub_kl, lem:bayesBinaryRisk_self}
We apply Corollary~\ref{cor:kl_estimation_ge_sub_kl} for the constant kernel with value $\xi$ and use Lemma~\ref{lem:bayesBinaryRisk_self}.
\end{proof}


\begin{theorem}[Fano's inequality, binary case]
  \label{thm:sub_bayesBinaryRisk_le_jensenShannon}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:jensenShannon}
  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\alpha \in (0, 1)$.
  \begin{align*}
  h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) \le \JS_\alpha(\mu, \nu) \: ,
  \end{align*}
  in which $h_2: x \mapsto x\log\frac{1}{x} + (1 - x)\log\frac{1}{1 - x}$ is the binary entropy function.
\end{theorem}

\begin{proof}%\leanok
\uses{lem:kl_estimation_ge_prod, thm:kl_compProd, lem:jensenShannon_eq_inf_kl}
Apply Lemma~\ref{lem:jensenShannon_eq_inf_kl} and then Lemma~\ref{lem:kl_estimation_ge_prod} with $\beta = 1/2$ to get
\begin{align*}
\JS_\alpha(\mu, \nu)
&= \inf_{\xi \in \mathcal P(\mathcal X)}\KL(\pi_\alpha \otimes P, \pi_\alpha \times \xi)
\\
&\ge \kl(B_\alpha(\mu, \nu), 1/2) - \kl(\alpha, 1/2)
\\
&= h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) \: .
\end{align*}
Note that $\beta = 1/2$ is the value that results in the best bound: the lower bound on $\JS_\alpha(\mu, \nu)$ from some $\beta \le 1/2$ would be
\begin{align*}
\kl(B_\alpha(\mu, \nu), \beta) - \kl(\alpha, \beta)
&= h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) - (\alpha - B_\alpha(\mu, \nu)) \log \frac{1 - \beta}{\beta}
\\
&\le h_2(\alpha) - h_2(B_\alpha(\mu, \nu)) \: .
\end{align*}

\end{proof}


\begin{theorem}
  \label{thm:log_inv_bayesBinaryRisk_le_renyi}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:Renyi}
  For $\alpha, \beta \in (0, 1/2)$,
  \begin{align*}
  \beta \log\frac{\alpha}{B_\alpha(\mu, \nu)} + (1 - \beta) \log\frac{1 - \alpha}{1 - B_\alpha(\mu, \nu)}
  &\le (1 - \beta) R_{\beta}(\mu, \nu)
  \: .
  \end{align*}
  As a consequence,
  \begin{align*}
  \beta \log\frac{\alpha}{B_\alpha(\mu, \nu)}
  &\le (1 - \beta) (R_{\beta}(\mu, \nu) + \log 2)
  \: .
  \end{align*}
  In particular, $\log\frac{\alpha}{B_\alpha(\mu, \nu)} \le R_{1/2}(\mu, \nu) + \log 2$~.
\end{theorem}

\begin{proof}%\leanok
\uses{lem:kl_estimation_ge_prod, lem:renyi_eq_inf_kl}
By Lemma~\ref{lem:renyi_eq_inf_kl} and then Lemma~\ref{lem:kl_estimation_ge_prod},
\begin{align*}
(1 - \beta) R_{\beta}(\mu, \nu)
&= \inf_{\xi \in \mathcal P(\mathcal X)} \KL(\pi_\beta \times \xi, \pi_\beta \otimes P)
\\
&\ge \kl(\beta, B_\alpha(\mu, \nu)) - \kl(\beta, \alpha)
\\
&= \beta \log\frac{\alpha}{B_\alpha(\mu, \nu)} + (1 - \beta)\log\frac{1 - \alpha}{1 - B_\alpha(\mu, \nu)}
\: .
\end{align*}
The first particular case is obtained by using $\alpha \le 1/2$ and $B_\pi(\mu, \nu) \ge 0$.
The second one further sets $\beta = 1/2$.
\end{proof}


%\begin{theorem}[TODO]
%  \label{thm:renyi_bayesBinaryRisk_le_renyi_mutual}
%  %\lean{}
%  %\leanok
%  \uses{def:bayesBinaryRisk, def:Renyi}
%  Let $\mu, \nu \in \mathcal P(\mathcal X)$ and let $\alpha \in (0, 1)$, $\lambda \in [0, +\infty)$. Let $r_\lambda(a,b)$ be the Rényi divergence between Bernoulli distributions with means $a$ and $b$.
%  \begin{align*}
%  r_\lambda(B_\alpha(\mu, \nu), \alpha) \le \inf_{\xi \in \mathcal P(\mathcal X)} R_\lambda(\pi_\alpha \otimes P, \pi_\alpha \times \xi) \: .
%  \end{align*}
%\end{theorem}



\paragraph{TODO: move or remove}


\begin{lemma}
  \label{lem:testing_bound_renyi_mean}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Let $\alpha \in (0,1)$. Then
  \begin{align*}
  \mu(E)^\alpha + \nu(E^c)^{1 - \alpha}
  \ge \exp\left(-(1 - \alpha) R_{\alpha}(\mu, \nu)\right)
  \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:renyi_data_proc_event}
Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$.
By Lemma~\ref{lem:renyi_data_proc_event}, $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$. That divergence is
\begin{align*}
R_\alpha(\mu_E, \nu_E)
&= \frac{1}{\alpha - 1}\log \left(\mu(E)^\alpha \nu(E)^{1 - \alpha}
  + \mu(E^c)^\alpha \nu(E^c)^{1 - \alpha}\right)
\\
&\ge \frac{1}{\alpha - 1}\log \left(\mu(E)^\alpha + \nu(E^c)^{1 - \alpha}\right)
\: .
\end{align*}
\end{proof}

\begin{corollary}
  \label{cor:testing_bound_hellinger}
  %\lean{}
  %\leanok
  \uses{def:Hellinger, def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Then
  \begin{align*}
  \sqrt{\mu(E)} + \sqrt{\nu(E^c)}
  \ge \exp\left(-\frac{1}{2} R_{1/2}(\mu, \nu)\right)
  = 1 - \Hsq(\mu, \nu)
  \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:testing_bound_renyi_mean, lem:renyi_half_eq_log_hellinger}
The inequality is an application of Lemma~\ref{lem:testing_bound_renyi_mean} for $\alpha = 1/2$. The equality is Lemma~\ref{lem:renyi_half_eq_log_hellinger}.
\end{proof}



\subsection{Lower bounds using the change of measure lemma}

The main tool of this section is the next lemma, which relates the probability of an event under two measures and deviations of the log-likelihood ratio between the measures.

\begin{lemma}[Change of measure lemma]
  \label{lem:llr_change_measure}
  \lean{ProbabilityTheory.measure_sub_le_measure_mul_exp'}
  \leanok
  %\uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$ with $\mu \ll \nu$ and let $E$ be an event on $\mathcal X$. Let $\beta \in \mathbb{R}$. Then
  \begin{align*}
  \nu(E) e^{\beta} \ge \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} > \beta \right\} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\begin{align*}
\nu(E)
\ge \mu\left[\mathbb{I}(E) e^{- \log\frac{d \mu}{d \nu} }\right]
&\ge \mu\left[\mathbb{I}\left(E \cap \left\{\log\frac{d \mu}{d \nu} \le \beta\right\}\right) e^{- \log\frac{d \mu}{d \nu} }\right]
\\
&\ge e^{- \beta}\mu\left(E \cap \left\{\log\frac{d \mu}{d \nu} \le \beta\right\}\right)
\\
&\ge e^{- \beta}\left( \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} > \beta \right\} \right)
\: .
\end{align*}
\end{proof}


\begin{lemma}[3 points change of measure]
  \label{lem:llr_change_measure_add}
  \lean{ProbabilityTheory.one_sub_le_add_measure_mul_exp}
  \leanok
  %\uses{}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\beta_1, \beta_2 \in \mathbb{R}$. Then
  \begin{align*}
  \mu(E) e^{\beta_1} + \nu(E^c) e^{\beta_2} \ge 1 - \xi\left\{ \log\frac{d \xi}{d \mu} > \beta_1 \right\} - \xi\left\{ \log\frac{d \xi}{d \nu} > \beta_2 \right\} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure}
Two applications of Lemma~\ref{lem:llr_change_measure}, then sum them and use $\xi(E)+\xi(E^c) = 1$.
\end{proof}



\paragraph{Change of measure and the moments of the log-likelihood ratio}

\begin{corollary}[Change of measure - mean]
  \label{cor:kl_change_measure}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\beta \in \mathbb{R}$. Then
  \begin{align*}
  \nu(E) e^{\KL(\mu, \nu) + \beta} \ge \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} - \KL(\mu, \nu) > \beta \right\} \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:llr_change_measure}
Use Lemma~\ref{lem:llr_change_measure} with the choice $\KL(\mu, \nu) + \beta$ for $\beta$.
\end{proof}

\begin{lemma}[Change of measure - variance]
  \label{lem:llr_change_measure_variance}
  %\lean{}
  %\leanok
  \uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$ such that $\mu\left[\left(\log\frac{d \mu}{d \nu}\right)^2\right] < \infty$. Let $E$ be an event on $\mathcal X$ and let $\beta > 0$. Then
  \begin{align*}
  \nu(E) e^{\KL(\mu, \nu) + \sqrt{\Var_\mu[\log\frac{d \mu}{d \nu}]\beta}} \ge \mu(E) - \frac{1}{\beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:llr_change_measure}
Use Lemma~\ref{lem:llr_change_measure} with the choice $\KL(\mu, \nu) + \sqrt{\Var_\mu[\log\frac{d \mu}{d \nu}]\beta}$ for $\beta$ and bound the probability of deviation of the log-likelihood ratio with Chebychev's inequality.
\end{proof}

\begin{lemma}[Change of measure - c.g.f.]
  \label{lem:renyi_chernoff_bound}
  \lean{ProbabilityTheory.measure_llr_gt_renyiDiv_le_exp}
  \leanok
  \uses{def:Renyi}
  For $\mu, \nu$ finite measures and $\alpha, \beta > 0$,
  \begin{align*}
  \mu\left\{ \log\frac{d \mu}{d \nu} > R_{1+\alpha}(\mu, \nu) + \beta \right\}
  \le e^{- \alpha \beta}
  \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:renyi_cgf_2}
This is a Chernoff bound, using that the cumulant generating function of $\log\frac{d\mu}{d\nu}$ under $\mu$ has value $\alpha R_{1+\alpha}(\mu, \nu)$ at $\alpha$ by Lemma~\ref{lem:renyi_cgf_2}.
\begin{align*}
\mu\left\{ \log\frac{d \mu}{d \nu} > R_{1+\alpha}(\mu, \nu) + \beta \right\}
&= \mu\left\{ \exp\left(\alpha\log\frac{d \mu}{d \nu}\right) > \exp\left(\alpha R_{1+\alpha}(\mu, \nu) + \alpha \beta\right) \right\}
\\
&\le e^{-\alpha R_{1+\alpha}(\mu, \nu) - \alpha \beta} \mu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha \right]
\end{align*}
Then $\mu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha \right] = \nu\left[\left(\frac{d \mu}{d \nu}\right)^{1+\alpha} \right] = e^{\alpha R_{1+\alpha}(\mu, \nu)}$.
\end{proof}

\begin{lemma}
  \label{lem:renyi_change_measure}
  \lean{ProbabilityTheory.measure_sub_le_measure_mul_exp_renyiDiv}
  \leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha,\beta > 0$. Then
  \begin{align*}
  \nu(E) e^{R_{1+\alpha}(\mu, \nu) + \beta} \ge \mu(E) - e^{-\alpha \beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure, lem:renyi_chernoff_bound}
Use Lemma~\ref{lem:llr_change_measure} with the choice $R_{1+\alpha}(\mu, \nu) + \beta$ for $\beta$. Then apply Lemma~\ref{lem:renyi_chernoff_bound}.
\end{proof}



\paragraph{Applications of the change of measure with 3 points}

\begin{lemma}
  \label{lem:change_measure_variance_add}
  % \lean{}
  % \leanok
  \uses{}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. For $\beta > 0$~,
  \begin{align*}
  \mu(E) e^{\KL(\xi, \mu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\mu}\right]}} + \nu(E^c) e^{\KL(\xi, \nu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\nu}\right]}}
  \ge 1 - \frac{2}{\beta} \: .
  \end{align*}
\end{lemma}

\begin{proof} %\leanok
\uses{lem:llr_change_measure_add}
Use Lemma~\ref{lem:llr_change_measure_add} with the choices $\KL(\xi, \mu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\mu}\right]}$ and $\KL(\xi, \nu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\nu}\right]}$ for $\beta_1$ and $\beta_2$.
Then use Chebyshev's inequality to bound the probabilities of deviation of the log-likelihood ratios.
\end{proof}

\begin{lemma}
  \label{lem:renyi_change_measure_add}
  \lean{ProbabilityTheory.one_sub_exp_le_add_measure_mul_exp_max_renyiDiv}
  \leanok
  \uses{def:Renyi}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha, \beta \ge 0$. Then
  \begin{align*}
  \mu(E) e^{R_{1+\alpha}(\xi, \mu) + \beta} + \nu(E^c) e^{R_{1+\alpha}(\xi, \nu) + \beta} \ge 1 - 2 e^{-\alpha \beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure_add, lem:renyi_chernoff_bound}
Use Lemma~\ref{lem:llr_change_measure_add} with the choices $R_{1+\alpha}(\xi, \mu) + \beta$ and $R_{1+\alpha}(\xi, \nu) + \beta$ for $\beta_1$ and $\beta_2$.
Then apply Lemma~\ref{lem:renyi_chernoff_bound}.
\end{proof}


\begin{lemma}
  \label{lem:testing_bound_renyi_one_add}
  \lean{ProbabilityTheory.exp_neg_chernoffDiv_le_add_measure}
  \leanok
  \uses{def:Renyi, def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha > 0$. Then
  \begin{align*}
  \mu(E) + \nu(E^c) \ge \frac{1}{2}\exp\left( - C_{1+\alpha}(\mu, \nu) - \frac{\log 4}{\alpha}\right) \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:renyi_change_measure_add}
Use Lemma~\ref{lem:renyi_change_measure_add} with $\beta = \log(4)/\alpha$ and use that
\begin{align*}
\mu(E) e^{R_{1+\alpha}(\xi, \mu) + \beta} + \nu(E^c) e^{R_{1+\alpha}(\xi, \nu) + \beta}
\le (\mu(E) + \nu(E^c)) e^{\max \{R_{1+\alpha}(\xi, \mu), R_{1+\alpha}(\xi, \nu)\} + \beta}
\: .
\end{align*}

\end{proof}



\paragraph{Product spaces}

\begin{lemma}
  \label{lem:testing_bound_renyi_one_add_n}
  %\lean{}
  %\leanok
  \uses{def:Renyi, def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$, let $n \in \mathbb{N}$ and let $E$ be an event on $\mathcal X^n$. For all $\alpha > 0$,
  \begin{align*}
  \mu^{\otimes n}(E) + \nu^{\otimes n}(E^c) \ge \frac{1}{2}\exp\left( - n C_{1+\frac{\alpha}{\sqrt{n}}}(\mu, \nu) - \frac{\log 4}{\alpha}\sqrt{n}\right) \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:renyi_prod_n, lem:testing_bound_renyi_one_add}
Use Lemma~\ref{lem:renyi_prod_n} in Lemma~\ref{lem:testing_bound_renyi_one_add}. TODO: add a lemma for tensorization of the Chernoff divergence.
\end{proof}

\begin{theorem}
  \label{thm:testing_bound_chernoff}
  %\lean{}
  %\leanok
  \uses{def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and let $(E_n)_{n \in \mathbb{N}}$ be events on $\mathcal X^n$. For all $\gamma \in (0,1)$,
  \begin{align*}
  \limsup_{n \to +\infty} \frac{1}{n}\log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)}
  \le C_1(\mu, \nu)
  \: .
  \end{align*}
\end{theorem}

\begin{proof}
\uses{cor:kl_change_measure, thm:kl_pi}
Let $\xi$ be a probability measure on $\mathcal X$ and $\beta > 0$. By Corollary~\ref{cor:kl_change_measure},
\begin{align*}
\mu^{\otimes n}(E_n) e^{\KL(\xi^{\otimes n}, \mu^{\otimes n}) + n\beta}
&\ge \xi^{\otimes n}(E_n) - \xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \mu^{\otimes n}} - \KL(\xi^{\otimes n}, \mu^{\otimes n}) > n\beta \right\}
\: , \\
\nu^{\otimes n}(E_n^c) e^{\KL(\xi^{\otimes n}, \nu^{\otimes n}) + n\beta}
&\ge \xi^{\otimes n}(E_n^c) - \xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \nu^{\otimes n}} - \KL(\xi^{\otimes n}, \nu^{\otimes n}) > n\beta \right\}
\: .
\end{align*}
We sum both inequalities with weights $\gamma$ and $1-\gamma$ respectively and use that each $\KL$ on the left is less than their max, as well as $\xi^{\otimes n}(E_n) + \xi^{\otimes n}(E_n^c) = 1$.
\begin{align*}
&e^{n\beta} (\gamma\mu^{\otimes n}(E_n) + (1-\gamma)\nu^{\otimes n}(E_n^c)) e^{\max\{\KL(\xi^{\otimes n}, \mu^{\otimes n}), \KL(\xi^{\otimes n}, \nu^{\otimes n})\}}
\\
&\ge \min\{\gamma, 1-\gamma\} - \gamma\xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \mu^{\otimes n}} - \KL(\xi^{\otimes n}, \mu^{\otimes n}) > n\beta \right\}
  - (1 - \gamma)\xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \nu^{\otimes n}} - \KL(\xi^{\otimes n}, \nu^{\otimes n}) > n\beta \right\}
\: .
\end{align*}
Let $p_{n,\mu}(\beta)$ and $p_{n, \nu}(\beta)$ be the two probabilities on the right hand side. By the law of large numbers, both tend to 0 when $n$ tends to $+\infty$.
In particular, for $n$ large enough, the right hand side is positive and we can take logarithms on both sides. We also use the tensorization of $\KL$ (Theorem~\ref{thm:kl_pi}).
\begin{align*}
& n \max\{\KL(\xi, \mu), \KL(\xi, \nu)\}
\\
&\ge \log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)} + \log (\min\{\gamma, 1-\gamma\} - \gamma p_{n, \mu}(\beta) - (1 - \gamma) p_{n, \nu}(\beta)) - n\beta
\end{align*}
For $n \to +\infty$,
\begin{align*}
\max\{\KL(\xi, \mu), \KL(\xi, \nu)\}
\ge \limsup_{n \to + \infty}\frac{1}{n}\log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)} - \beta
\end{align*}
Since $\beta > 0$ is arbitrary, we can take a supremum over $\beta$ on the right.
\end{proof}


\section{Sample complexity}

An estimation problem of the shape $(P^{\otimes n}, y, \ell')$ corresponds to estimating from $n$ i.i.d. samples of the distribution. From the data-processing inequality, we get that the risk (Bayes risk, minimax risk...) decreases with the number of samples (Lemma~\ref{lem:bayesRisk_mono_prod} for example).
Intuitively, having more information allows for better estimation.

In a sequence of estimation tasks $(P^{\otimes n}, y, \ell')$ for $n \in \mathbb{N}$, the \emph{sample complexity} is the minimal $n$ such that the risk is less than a desired value.

\begin{lemma}
  \label{lem:bayesRisk_mono_prod}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  If $n \le m$ then $\mathcal R_\pi^{P^{\otimes n}} \ge \mathcal R_\pi^{P^{\otimes m}}$.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:data_proc_bayesRisk}
Use the data-processing inequality, Theorem~\ref{thm:data_proc_bayesRisk}.
\end{proof}


\begin{definition}
  \label{def:priorSampleComplexity}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  The sample complexity of Bayesian estimation with respect to a prior $\pi \in \mathcal M(\Theta)$ at risk level $\delta \in \mathbb{R}_{+,\infty}$ is
  \begin{align*}
  n_\pi^P(\delta) = \min \{n \in \mathbb{N} \mid \mathcal R_\pi^{P^{\otimes n}} \le \delta\} \: .
  \end{align*}
\end{definition}


\begin{definition}
  \label{def:binaryPriorSampleComplexity}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:priorSampleComplexity}
  The sample complexity of simple binary hypothesis testing with prior $(\pi, 1 - \pi) \in \mathcal P(\{0, 1\})$ at risk level $\delta \in \mathbb{R}_{+, \infty}$ is
  \begin{align*}
  n(\mu, \nu, \pi, \delta) = \min\{n \in \mathbb{N} \mid B_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \le \delta\} \: .
  \end{align*}
  This is the sample complexity $n_\xi^P(\delta)$ specialized to simple binary hypothesis testing.
\end{definition}


\begin{lemma}
  \label{lem:binaryPriorSampleComplexity_eq_zero}
  %\lean{}
  %\leanok
  \uses{def:binaryPriorSampleComplexity}
  For $\delta \ge \min\{\pi, 1 - \pi\}$, the sample complexity of simple binary hypothesis testing is
  $n(\mu, \nu, \pi, \delta) = 0$~.
\end{lemma}

\begin{proof}
\uses{lem:bayesBinaryRisk_le}
\end{proof}


\begin{lemma}
  \label{lem:binaryPriorSampleComplexity_le_renyi}
  %\lean{}
  %\leanok
  \uses{def:Renyi, def:binaryPriorSampleComplexity}
  The sample complexity of simple binary hypothesis testing satisfies $n(\mu, \nu, \pi, \delta) \le n_0$~, with $n_0$ the smallest natural number such that
  \begin{align*}
  \log\frac{1}{\delta}
  \le \sup_{\alpha \in (0,1)} \left( n_0 (1 - \alpha)R_\alpha(\mu, \nu) + \alpha\log\frac{1}{\pi} + (1 - \alpha)\log\frac{1}{1 - \pi} \right)
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{cor:bayesBinaryRisk_le_exp_renyi, lem:renyi_prod_n}
It suffices to show that $B_\pi(\mu^{\otimes n_0}, \nu^{\otimes n_0}) \le \delta$~.
By Corollary~\ref{cor:bayesBinaryRisk_le_exp_renyi}, for all $\alpha \in (0,1)$,
\begin{align*}
B_\pi(\mu^{\otimes n_0}, \nu^{\otimes n_0})
&\le \pi^{\alpha} (1 - \pi)^{1 - \alpha}e^{- (1 - \alpha)R_\alpha(\mu^{\otimes n_0}, \nu^{\otimes n_0})}
\: .
\end{align*}
The Rényi divergence tensorizes: $R_\alpha(\mu^{\otimes n_0}, \nu^{\otimes n_0}) = n_0 R_\alpha(\mu, \nu)$~.
\begin{align*}
B_\pi(\mu^{\otimes n_0}, \nu^{\otimes n_0})
&\le \exp\left(- \left( n_0 (1 - \alpha)R_\alpha(\mu, \nu) + \alpha\log\frac{1}{\pi} + (1 - \alpha)\log\frac{1}{1 - \pi} \right) \right)
\: .
\end{align*}
By definition, $n_0$ is such that the infimum over $\alpha$ of the right hand side is less than $\delta$.
\end{proof}


\begin{lemma}
  \label{lem:binaryPriorSampleComplexity_ge_renyi}
  %\lean{}
  %\leanok
  \uses{def:binaryPriorSampleComplexity, def:Renyi}
  For $\delta \le \pi \le 1/2$, the sample complexity of simple binary hypothesis testing satisfies
  \begin{align*}
  n(\mu, \nu, \pi, \delta) \ge \frac{\log\frac{\pi}{2\delta}}{R_{1/2}(\mu, \nu)}
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{thm:log_inv_bayesBinaryRisk_le_renyi, lem:renyi_prod_n}
By Theorem~\ref{thm:log_inv_bayesBinaryRisk_le_renyi},
\begin{align*}
\log\frac{\pi}{B_\pi(\mu^{\otimes n}, \nu^{\otimes n})} \le R_{1/2}(\mu^{\otimes n}, \nu^{\otimes n}) + \log 2
\: .
\end{align*}
The Rényi divergence tensorizes (Theorem~\ref{lem:renyi_prod_n}): $R_{1/2}(\mu^{\otimes n}, \nu^{\otimes n}) = n R_{1/2}(\mu, \nu)$~.
We finally use that $B_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \le \delta$ for $n = n(\mu, \nu, \pi, \delta)$~.
\end{proof}


\begin{lemma}
  \label{lem:binaryPriorSampleComplexity_ge_jensenShannon}
  %\lean{}
  %\leanok
  \uses{def:binaryPriorSampleComplexity, def:jensenShannon}
  For $\delta \le \min\{\pi, 1 - \pi\}$, the sample complexity of simple binary hypothesis testing satisfies
  \begin{align*}
  n(\mu, \nu, \pi, \delta) \ge \frac{h_2(\pi) - h_2(\delta)}{\JS_\pi(\mu, \nu)}
  \: ,
  \end{align*}
  in which $h_2: x \mapsto x\log\frac{1}{x} + (1 - x)\log\frac{1}{1 - x}$ is the binary entropy function.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:sub_bayesBinaryRisk_le_jensenShannon}
We start from Theorem~\ref{thm:sub_bayesBinaryRisk_le_jensenShannon}.
\begin{align*}
\JS_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \ge h_2(\pi) - h_2(B_\pi(\mu^{\otimes n}, \nu^{\otimes n})) \: .
\end{align*}
By Lemma~\ref{lem:jensenShannon_prod_n}, $\JS_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \le n \JS_\pi(\mu, \nu)$. The final result is obtained by using that $B_\pi(\mu^{\otimes n}, \nu^{\otimes n}) \le \delta$ for $n = n(\mu, \nu, \pi, \delta)$.
\end{proof}