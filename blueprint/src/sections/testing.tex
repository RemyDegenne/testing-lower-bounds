\chapter{Testing error lower bounds}

In simple hypothesis testing, a sample from an unknown measure on $\mathcal X$ among $\{\mu, \nu\}$ is observed, and the goal of the test is to return the measure from which the sample came.
In typical scenarios, $\mathcal X$ is a product space $\mathcal Y^n$, $\mu$ and $\nu$ are product measures and the observation is a collection of $n$ i.i.d. samples.

A test is a measurable function $\phi : \mathcal X \to \{0,1\}$, which can equivalently be described by the even $E = \{\phi = 1\}$. If we decide that the test should return 0 if the sample came from $\mu$ and 1 if it came from $\nu$, the two error probabilities of the test are $\mu(E)$ and $\nu(E^c)$. A good test minimizes these (or a given combination of these).

In this section, we prove lower bounds on functions of $\mu(E)$ and $\nu(E^c)$, for any event $E$.
For the sum $\mu(E) + \nu(E^c)$, the value of the optimal test is exactly $1 - \TV(\mu, \nu)$, as shown in the following theorem.

\begin{theorem}
  \label{thm:testing_eq_tv}
  %\lean{}
  %\leanok
  \uses{def:TV}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$. Then $\inf_{E \text{ event}}\{\mu(E) + \nu(E^c)\} = 1 - \TV(\mu, \nu)$~.
\end{theorem}

\begin{proof}
\uses{thm:tv_eq_sup_sub_measure}
By Theorem~\ref{thm:tv_eq_sup_sub_measure},
\begin{align*}
-TV(\mu, \nu)
= -\sup_{E \text{ event}} \left( \mu(E) - \nu(E) \right)
=\inf_{E \text{ event}} \left(\nu(E) - \mu(E) \right)
= \inf_{E \text{ event}} \left( \nu(E) + \mu(E^c) \right) - 1
\: .
\end{align*}
\end{proof}

The drawback of $\TV$ is that it is not easy to manipulate. In particular, it does not tensorize. In order to quantify error probabilities on product spaces like $\mu^{\otimes n}(E) + \nu^{\otimes n}(E^c)$, we use other divergences.

\section{Generic lower bounds}

\begin{lemma}
  \label{lem:testing_bound_renyi_mean}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Let $\alpha \in (0,1)$. Then
  \begin{align*}
  \mu(E)^\alpha + \nu(E^c)^{1 - \alpha}
  \ge \exp\left(-(1 - \alpha) R_{\alpha}(\mu, \nu)\right)
  \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:renyi_data_proc_event}
Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$.
By Lemma~\ref{lem:renyi_data_proc_event}, $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$. That divergence is
\begin{align*}
R_\alpha(\mu_E, \nu_E)
&= \frac{1}{\alpha - 1}\log \left(\mu(E)^\alpha \nu(E)^{1 - \alpha}
  + \mu(E^c)^\alpha \nu(E^c)^{1 - \alpha}\right)
\\
&\ge \frac{1}{\alpha - 1}\log \left(\mu(E)^\alpha + \nu(E^c)^{1 - \alpha}\right)
\: .
\end{align*}
\end{proof}

\begin{corollary}
  \label{cor:testing_bound_hellinger}
  %\lean{}
  %\leanok
  \uses{def:Hellinger, def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Then
  \begin{align*}
  \sqrt{\mu(E)} + \sqrt{\nu(E^c)}
  \ge \exp\left(-\frac{1}{2} R_{1/2}(\mu, \nu)\right)
  = 1 - \Hsq(\mu, \nu)
  \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:testing_bound_renyi_mean, lem:renyi_half_eq_log_hellinger}
The inequality is an application of Lemma~\ref{lem:testing_bound_renyi_mean} for $\alpha = 1/2$. The equality is Lemma~\ref{lem:renyi_half_eq_log_hellinger}.
\end{proof}

\section{Change of measure}

\begin{lemma}
  \label{lem:llr_change_measure}
  \lean{ProbabilityTheory.measure_sub_le_measure_mul_exp'}
  \leanok
  %\uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$ with $\mu \ll \nu$ and let $E$ be an event on $\mathcal X$. Let $\beta \in \mathbb{R}$. Then
  \begin{align*}
  \nu(E) e^{\beta} \ge \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} > \beta \right\} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\begin{align*}
\nu(E)
\ge \mu\left[\mathbb{I}(E) e^{- \log\frac{d \mu}{d \nu} }\right]
&\ge \mu\left[\mathbb{I}\left(E \cap \left\{\log\frac{d \mu}{d \nu} \le \beta\right\}\right) e^{- \log\frac{d \mu}{d \nu} }\right]
\\
&\ge e^{- \beta}\mu\left(E \cap \left\{\log\frac{d \mu}{d \nu} \le \beta\right\}\right)
\\
&\ge e^{- \beta}\left( \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} > \beta \right\} \right)
\: .
\end{align*}
\end{proof}

\begin{corollary}
  \label{cor:kl_change_measure}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\beta \in \mathbb{R}$. Then
  \begin{align*}
  \nu(E) e^{\KL(\mu, \nu) + \beta} \ge \mu(E) - \mu\left\{ \log\frac{d \mu}{d \nu} - \KL(\mu, \nu) > \beta \right\} \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:llr_change_measure}
Use Lemma~\ref{lem:llr_change_measure} with the choice $\KL(\mu, \nu) + \beta$ for $\beta$.
\end{proof}

\begin{lemma}
  \label{lem:llr_change_measure_variance}
  %\lean{}
  %\leanok
  \uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$ such that $\mu\left[\left(\log\frac{d \mu}{d \nu}\right)^2\right] < \infty$. Let $E$ be an event on $\mathcal X$ and let $\beta > 0$. Then
  \begin{align*}
  \nu(E) e^{\KL(\mu, \nu) + \sqrt{\Var_\mu[\log\frac{d \mu}{d \nu}]\beta}} \ge \mu(E) - \frac{1}{\beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:llr_change_measure}
Use Lemma~\ref{lem:llr_change_measure} with the choice $\KL(\mu, \nu) + \sqrt{\Var_\mu[\log\frac{d \mu}{d \nu}]\beta}$ for $\beta$ and bound the probability of deviation of the log-likelihood ratio with Chebychev's inequality.
\end{proof}

\begin{lemma}
  \label{lem:renyi_chernoff_bound}
  \lean{ProbabilityTheory.measure_llr_gt_renyiDiv_le_exp}
  \leanok
  \uses{def:Renyi}
  For $\mu, \nu$ finite measures and $\alpha, \beta > 0$,
  \begin{align*}
  \mu\left\{ \log\frac{d \mu}{d \nu} > R_{1+\alpha}(\mu, \nu) + \beta \right\}
  \le e^{- \alpha \beta}
  \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:renyi_cgf_2}
This is a Chernoff bound, using that the cumulant generating function of $\log\frac{d\mu}{d\nu}$ under $\mu$ has value $\alpha R_{1+\alpha}(\mu, \nu)$ at $\alpha$ by Lemma~\ref{lem:renyi_cgf_2}.
\begin{align*}
\mu\left\{ \log\frac{d \mu}{d \nu} > R_{1+\alpha}(\mu, \nu) + \beta \right\}
&= \mu\left\{ \exp\left(\alpha\log\frac{d \mu}{d \nu}\right) > \exp\left(\alpha R_{1+\alpha}(\mu, \nu) + \alpha \beta\right) \right\}
\\
&\le e^{-\alpha R_{1+\alpha}(\mu, \nu) - \alpha \beta} \mu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha \right]
\end{align*}
Then $\mu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha \right] = \nu\left[\left(\frac{d \mu}{d \nu}\right)^{1+\alpha} \right] = e^{\alpha R_{1+\alpha}(\mu, \nu)}$.
\end{proof}

\begin{lemma}
  \label{lem:renyi_change_measure}
  \lean{ProbabilityTheory.measure_sub_le_measure_mul_exp_renyiDiv}
  \leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha,\beta > 0$. Then
  \begin{align*}
  \nu(E) e^{R_{1+\alpha}(\mu, \nu) + \beta} \ge \mu(E) - e^{-\alpha \beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure, lem:renyi_chernoff_bound}
Use Lemma~\ref{lem:llr_change_measure} with the choice $R_{1+\alpha}(\mu, \nu) + \beta$ for $\beta$. Then apply Lemma~\ref{lem:renyi_chernoff_bound}.
\end{proof}

\begin{lemma}
  \label{lem:llr_change_measure_add}
  \lean{ProbabilityTheory.one_sub_le_add_measure_mul_exp}
  \leanok
  %\uses{}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\beta_1, \beta_2 \in \mathbb{R}$. Then
  \begin{align*}
  \mu(E) e^{\beta_1} + \nu(E^c) e^{\beta_2} \ge 1 - \xi\left\{ \log\frac{d \xi}{d \mu} > \beta_1 \right\} - \xi\left\{ \log\frac{d \xi}{d \nu} > \beta_2 \right\} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure}
Two applications of Lemma~\ref{lem:llr_change_measure}, then sum them and use $\xi(E)+\xi(E^c) = 1$.
\end{proof}

\begin{lemma}
  \label{lem:change_measure_variance_add}
  % \lean{}
  % \leanok
  \uses{}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. For $\beta > 0$~,
  \begin{align*}
  \mu(E) e^{\KL(\xi, \mu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\mu}\right]}} + \nu(E^c) e^{\KL(\xi, \nu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\nu}\right]}}
  \ge 1 - \frac{2}{\beta} \: .
  \end{align*}
\end{lemma}

\begin{proof} %\leanok
\uses{lem:llr_change_measure_add}
Use Lemma~\ref{lem:llr_change_measure_add} with the choices $\KL(\xi, \mu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\mu}\right]}$ and $\KL(\xi, \nu) + \sqrt{\beta \Var_{\xi}\left[\log\frac{d\xi}{d\nu}\right]}$ for $\beta_1$ and $\beta_2$.
Then use Chebyshev's inequality to bound the probabilities of deviation of the log-likelihood ratios.
\end{proof}

\begin{lemma}
  \label{lem:renyi_change_measure_add}
  \lean{ProbabilityTheory.one_sub_exp_le_add_measure_mul_exp_max_renyiDiv}
  \leanok
  \uses{def:Renyi}
  Let $\mu, \nu, \xi$ be three probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha, \beta \ge 0$. Then
  \begin{align*}
  \mu(E) e^{R_{1+\alpha}(\xi, \mu) + \beta} + \nu(E^c) e^{R_{1+\alpha}(\xi, \nu) + \beta} \ge 1 - 2 e^{-\alpha \beta} \: .
  \end{align*}
\end{lemma}

\begin{proof}\leanok
\uses{lem:llr_change_measure_add, lem:renyi_chernoff_bound}
Use Lemma~\ref{lem:llr_change_measure_add} with the choices $R_{1+\alpha}(\xi, \mu) + \beta$ and $R_{1+\alpha}(\xi, \nu) + \beta$ for $\beta_1$ and $\beta_2$.
Then apply Lemma~\ref{lem:renyi_chernoff_bound}.
\end{proof}


\section{Lower bounds on the maximum}

\begin{lemma}
  \label{lem:testing_bound_tv_hellinger_max}
  %\lean{}
  %\leanok
  \uses{def:TV, def:Hellinger}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Then
  \begin{align*}
  \max\{\mu(E), \nu(E^c)\}
  \ge \frac{1}{2}(1 - \TV(\mu, \nu))
  \ge \frac{1}{4}(1 - \Hsq(\mu, \nu))^2
  \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{thm:testing_eq_tv, cor:one_sub_hellinger_squared_le_one_sub_tv}
The first inequality is Theorem~\ref{thm:testing_eq_tv} with $\mu(E) + \nu(E^c) \le 2 \max\{\mu(E), \nu(E^c)\}$.
The second inequality is a consequence of Corollary~\ref{cor:one_sub_hellinger_squared_le_one_sub_tv}.
\end{proof}

\begin{lemma}
  \label{lem:testing_bound_renyi_max}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event. Let $\alpha \in (0,1)$. Then
  \begin{align*}
  \min\{\alpha, 1 - \alpha\} \log\frac{1}{\max\{\mu(E), \nu(E^c)\}} \le (1 - \alpha) R_{\alpha}(\mu, \nu)  + \log 2 \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:testing_bound_renyi_mean}
Use Lemma~\ref{lem:testing_bound_renyi_mean} and remark that $\mu(E)^\alpha + \nu(E^c)^{1 - \alpha} \le 2\max\{\mu(E), \nu(E^c)\}^{\min\{\alpha, 1 - \alpha\}}$.
\end{proof}


\begin{lemma}
  \label{lem:testing_bound_renyi_one_add}
  \lean{ProbabilityTheory.exp_neg_chernoffDiv_le_add_measure}
  \leanok
  \uses{def:Renyi, def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and let $E$ be an event on $\mathcal X$. Let $\alpha > 0$. Then
  \begin{align*}
  \mu(E) + \nu(E^c) \ge \frac{1}{2}\exp\left( - C_{1+\alpha}(\mu, \nu) - \frac{\log 4}{\alpha}\right) \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:renyi_change_measure_add}
\end{proof}


\section{Product spaces}

\begin{corollary}
  \label{cor:testing_bound_renyi_n}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and $E$ an event of $\mathcal X^n$. Let $\alpha \in (0,1)$. Then
  \begin{align*}
  \min\{\alpha, 1 - \alpha\} \log\frac{1}{\max\{\mu^{\otimes n}(E), \nu^{\otimes n}(E^c)\}} \le (1 - \alpha) n R_{\alpha}(\mu, \nu)  + \log 2 \: .
  \end{align*}
\end{corollary}

\begin{proof}
\uses{lem:testing_bound_renyi_max, lem:renyi_prod_n}
Use Lemma~\ref{lem:renyi_prod_n} in Lemma~\ref{lem:testing_bound_renyi_max}.
\end{proof}

\begin{lemma}
  \label{lem:testing_bound_renyi_one_add_n}
  %\lean{}
  %\leanok
  \uses{def:Renyi, def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$, let $n \in \mathbb{N}$ and let $E$ be an event on $\mathcal X^n$. For all $\alpha > 0$,
  \begin{align*}
  \mu^{\otimes n}(E) + \nu^{\otimes n}(E^c) \ge \frac{1}{2}\exp\left( - n C_{1+\frac{\alpha}{\sqrt{n}}}(\mu, \nu) - \frac{\log 4}{\alpha}\sqrt{n}\right) \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{lem:renyi_prod_n, lem:testing_bound_renyi_one_add}
Use Lemma~\ref{lem:renyi_prod_n} in Lemma~\ref{lem:testing_bound_renyi_one_add}. TODO: add a lemma for tensorization of the Chernoff divergence.
\end{proof}

\begin{theorem}
  \label{thm:testing_bound_chernoff}
  %\lean{}
  %\leanok
  \uses{def:Chernoff}
  Let $\mu, \nu$ be two probability measures on $\mathcal X$ and let $(E_n)_{n \in \mathbb{N}}$ be events on $\mathcal X^n$. For all $\gamma \in (0,1)$,
  \begin{align*}
  \limsup_{n \to +\infty} \frac{1}{n}\log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)}
  \le C_1(\mu, \nu)
  \: .
  \end{align*}
\end{theorem}

\begin{proof}
\uses{cor:kl_change_measure, thm:kl_pi}
Let $\xi$ be a probability measure on $\mathcal X$ and $\beta > 0$. By Corollary~\ref{cor:kl_change_measure},
\begin{align*}
\mu^{\otimes n}(E_n) e^{\KL(\xi^{\otimes n}, \mu^{\otimes n}) + n\beta}
&\ge \xi^{\otimes n}(E_n) - \xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \mu^{\otimes n}} - \KL(\xi^{\otimes n}, \mu^{\otimes n}) > n\beta \right\}
\: , \\
\nu^{\otimes n}(E_n^c) e^{\KL(\xi^{\otimes n}, \nu^{\otimes n}) + n\beta}
&\ge \xi^{\otimes n}(E_n^c) - \xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \nu^{\otimes n}} - \KL(\xi^{\otimes n}, \nu^{\otimes n}) > n\beta \right\}
\: .
\end{align*}
We sum both inequalities with weights $\gamma$ and $1-\gamma$ respectively and use that each $\KL$ on the left is less than their max, as well as $\xi^{\otimes n}(E_n) + \xi^{\otimes n}(E_n^c) = 1$.
\begin{align*}
&e^{n\beta} (\gamma\mu^{\otimes n}(E_n) + (1-\gamma)\nu^{\otimes n}(E_n^c)) e^{\max\{\KL(\xi^{\otimes n}, \mu^{\otimes n}), \KL(\xi^{\otimes n}, \nu^{\otimes n})\}}
\\
&\ge \min\{\gamma, 1-\gamma\} - \gamma\xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \mu^{\otimes n}} - \KL(\xi^{\otimes n}, \mu^{\otimes n}) > n\beta \right\}
  - (1 - \gamma)\xi^{\otimes n}\left\{ \log\frac{d \xi^{\otimes n}}{d \nu^{\otimes n}} - \KL(\xi^{\otimes n}, \nu^{\otimes n}) > n\beta \right\}
\: .
\end{align*}
Let $p_{n,\mu}(\beta)$ and $p_{n, \nu}(\beta)$ be the two probabilities on the right hand side. By the law of large numbers, both tend to 0 when $n$ tends to $+\infty$.
In particular, for $n$ large enough, the right hand side is positive and we can take logarithms on both sides. We also use the tensorization of $\KL$ (Theorem~\ref{thm:kl_pi}).
\begin{align*}
& n \max\{\KL(\xi, \mu), \KL(\xi, \nu)\}
\\
&\ge \log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)} + \log (\min\{\gamma, 1-\gamma\} - \gamma p_{n, \mu}(\beta) - (1 - \gamma) p_{n, \nu}(\beta)) - n\beta
\end{align*}
For $n \to +\infty$,
\begin{align*}
\max\{\KL(\xi, \mu), \KL(\xi, \nu)\}
\ge \limsup_{n \to + \infty}\frac{1}{n}\log \frac{1}{\gamma \mu^{\otimes n}(E_n) + (1 - \gamma)\nu^{\otimes n}(E_n^c)} - \beta
\end{align*}
Since $\beta > 0$ is arbitrary, we can take a supremum over $\beta$ on the right.
\end{proof}